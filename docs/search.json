[
  {
    "objectID": "about_me/dcu_list/vermont.html",
    "href": "about_me/dcu_list/vermont.html",
    "title": "SG-FECC Burlington, Vermont, USA üá∫üá∏",
    "section": "",
    "text": "Participating in the Schlesinger Global Family Enterprise Case Competition (SG-FECC) was a defining moment in both my academic and personal development. It was my first time traveling to the United States, and the experience of competing at an international level not only challenged me but also provided invaluable insights into teamwork, strategic thinking, and professional growth.\nOur team represented Dublin City University (DCU) in the undergraduate Oak Division, where we competed against universities from around the world. Over three days, we tackled complex case studies, refining our ability to analyse real-world business challenges under pressure. Through collaboration, strategic discussions, and continuous adaptation, we achieved a perfect score (16 points out of 16) securing first place in our division.\nBeyond the competition itself, the experience was shaped by the people who supported us. Catherine Faherty and Eric Clinton played a crucial role in preparing us for success, providing mentorship, training, and invaluable guidance. Their dedication was instrumental in refining our approach and helping us perform at our best. Additionally, the support from Dublin City University enabled us to take part in this prestigious competition.\nWorking alongside my teammates, David, Sarah, and Evelyn, was one of the most rewarding aspects of this experience. From our initial practice cases to standing at the top of our division, we pushed each other to improve, supported one another in high-pressure moments, and developed a strong sense of camaraderie. The teamwork and problem-solving skills we honed throughout the competition will undoubtedly serve us well in our future careers.\nA special mention must also go to Aiden, who ensured we had everything we needed during our time in Burlington. His role as an ambassador made the experience smoother and more enjoyable, allowing us to focus on performing at our best.\nReflecting on this experience, I recognise how it has shaped my ability to think critically, work effectively in a team, and present solutions with confidence. The opportunity to compete internationally, engage with peers from different backgrounds, and apply my skills in a real-world setting was invaluable. I hope that future DCU students can continue to build on this success and benefit from this incredible learning experience.\nThis competition reinforced the importance of resilience, adaptability, and strategic decision-making, and I am eager to apply these lessons in future academic and professional endeavors. üá∫üá∏üèÜ"
  },
  {
    "objectID": "about_me/dcu_list/berlin.html",
    "href": "about_me/dcu_list/berlin.html",
    "title": "BIP Berlin, Germany üá©üá™",
    "section": "",
    "text": "I had the incredible honour of representing Dublin City University at the BIP Berlin Entrepreneur Bootcamp, after being selected among 14 other final-year business school students‚Äîa distinction that filled me with pride and gratitude. This intensive, week-long program was hosted collaboratively by The Berlin School of Economics and Law, The Drivery innovation hub, and the Startup Incubator Berlin. The bootcamp united passionate students and aspiring entrepreneurs from three esteemed academic institutions: Metropolia University of Applied Sciences in Helsinki, The Berlin School of Economics and Law, and Dublin City University. This international collaboration provided a vibrant and dynamic atmosphere that encouraged innovation, cultural exchange, and entrepreneurial creativity.\n\nThroughout the bootcamp, I had the opportunity to collaborate closely with a talented, diverse, and highly motivated team of students from Finland and Germany. Together, we addressed pressing sustainability challenges faced by modern cities, exploring ways to foster meaningful and sustainable behavioural change among visitors to Berlin. Partnering directly with the sustainability-focused startup 2zero, our team developed an innovative, incentive-based reward system aimed at promoting eco-friendly practices among tourists. By combining creative problem-solving, user-centric design, and sustainable business models, we aimed to encourage tourists to choose environmentally responsible options during their visits.\n\nAs the project unfolded, I was privileged to take on the responsibility of presenting our solution to an esteemed panel of judges, mentors, and industry experts. After a competitive process, our idea was selected as the winning entry for the 2zero Sustainability Challenge. Achieving this recognition among many innovative projects validated our hard work and affirmed the potential positive impact of our solution in real-world contexts.\nAlthough our days were filled with collaborative discussions and team projects, we also had the opportunity to immerse ourselves in the city and experience its vibrant nightlife. These moments allowed us to build strong connections and foster lasting friendships with everyone involved.\nOverall, the BIP Berlin Entrepreneur Bootcamp was an unforgettable experience, equipping me with practical entrepreneurial skills, intercultural insights, and meaningful relationships that will undoubtedly serve me well in my future career. It reaffirmed my passion for entrepreneurship, sustainability, and international collaboration, marking a highlight of my academic journey."
  },
  {
    "objectID": "about_me/kkr.html",
    "href": "about_me/kkr.html",
    "title": "Time at KKR üíº",
    "section": "",
    "text": "My 15-month internship with KKR‚Äôs Credit Strategies Reporting / Client Reporting Team was an experience that significantly shaped my professional growth and skill set. Coming into the role, I was initially nervous, but from day one, I was entrusted with responsibilities that made me feel like more than just an intern, I was a valued member of the team.\nThe experience allowed me to develop a strong analytical skillset, refine my ability to work with complex financial data, and gain exposure to the fast-paced world of Leveraged Credit. Whether it was crafting investment reports, automating data processes, or responding to investor queries, I was continuously challenged to think critically and improve operational efficiency.\nI am deeply grateful to Jennifer Corbett and Diarmaid Nash for giving me the opportunity to join the team and believing in my potential. Their decision to bring me on board has played a pivotal role in shaping my career trajectory, and to Harry Knowles and Anjali Singh, whose mentorship and guidance provided me with invaluable insights into the industry and helped me navigate my role with confidence.\nA defining aspect of my time at KKR was the firm‚Äôs culture, which is deeply rooted in the partnership and lifelong friendship of its founders, Henry Kravis and George Roberts. From day one, I experienced a level of inclusivity, respect, and camaraderie that made my internship truly special. Whether it was team outings like watching an Ireland soccer match, one-on-one coffee chats, or even the team coming out to support me at a charity boxing match, it was the people who made this experience so memorable."
  },
  {
    "objectID": "about_me/kkr.html#reflecting-on-my-time-at-kkr",
    "href": "about_me/kkr.html#reflecting-on-my-time-at-kkr",
    "title": "Time at KKR üíº",
    "section": "",
    "text": "My 15-month internship with KKR‚Äôs Credit Strategies Reporting / Client Reporting Team was an experience that significantly shaped my professional growth and skill set. Coming into the role, I was initially nervous, but from day one, I was entrusted with responsibilities that made me feel like more than just an intern, I was a valued member of the team.\nThe experience allowed me to develop a strong analytical skillset, refine my ability to work with complex financial data, and gain exposure to the fast-paced world of Leveraged Credit. Whether it was crafting investment reports, automating data processes, or responding to investor queries, I was continuously challenged to think critically and improve operational efficiency.\nI am deeply grateful to Jennifer Corbett and Diarmaid Nash for giving me the opportunity to join the team and believing in my potential. Their decision to bring me on board has played a pivotal role in shaping my career trajectory, and to Harry Knowles and Anjali Singh, whose mentorship and guidance provided me with invaluable insights into the industry and helped me navigate my role with confidence.\nA defining aspect of my time at KKR was the firm‚Äôs culture, which is deeply rooted in the partnership and lifelong friendship of its founders, Henry Kravis and George Roberts. From day one, I experienced a level of inclusivity, respect, and camaraderie that made my internship truly special. Whether it was team outings like watching an Ireland soccer match, one-on-one coffee chats, or even the team coming out to support me at a charity boxing match, it was the people who made this experience so memorable."
  },
  {
    "objectID": "about_me/kkr.html#key-contributions-skills-developed",
    "href": "about_me/kkr.html#key-contributions-skills-developed",
    "title": "Time at KKR üíº",
    "section": "Key Contributions & Skills Developed",
    "text": "Key Contributions & Skills Developed\nThroughout my internship, I had the opportunity to work on a variety of projects that enhanced both my technical and analytical capabilities:\n‚Ä¢ Investment Reporting & Data Analysis\n‚Ä¢ Compiled detailed investment reports for Credit funds, utilising multiple data sources to provide insights on performance and key metrics.\n‚Ä¢ Demonstrated strong analytical acumen and meticulous attention to detail in presenting investment information to stakeholders.\n‚Ä¢ Process Automation & Efficiency Improvements\n‚Ä¢ Developed automation techniques to streamline Excel-based data collection, significantly reducing reporting time.\n‚Ä¢ Integrated UpSlide to enhance the transfer of data between Excel and reports, improving overall workflow efficiency.\n‚Ä¢ Investor & Internal Reporting\n‚Ä¢ Managed Credit Investment headcount queries and prepared comprehensive reports on investment personnel.\n‚Ä¢ Assisted in completing Leveraged Credit RFPs (Requests for Proposal) and DDQs (Due Diligence Questionnaires), ensuring timely and accurate responses to investor inquiries\n‚Ä¢ Created templates for fund performance data collection on Zephyr, facilitating a smooth transition from StyleAdvisor to a modern digital platform\n‚Ä¢ Stakeholder Engagement & Problem-Solving\n‚Ä¢ Responded to ad hoc investor and internal queries, demonstrating adaptability and a solution-oriented mindset in addressing data-related challenges"
  },
  {
    "objectID": "about_me/kkr.html#looking-ahead",
    "href": "about_me/kkr.html#looking-ahead",
    "title": "Time at KKR üíº",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nReflecting on this experience, I recognise how much I have grown professionally and personally. The exposure to complex financial reporting, automation tools, and investor relations has equipped me with invaluable skills that I will carry forward in my career. More importantly, the relationships I built and the collaborative team environment at KKR have left a lasting impact on me.\nAs I move forward, I am excited to apply the knowledge and skills gained from this experience while continuing to develop as a finance professional. I look forward to staying connected with the incredible people I met along the way and seeing where the next chapter of my career takes me."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects During College üíª",
    "section": "",
    "text": "Throughout my time in college, I have worked on a variety of assignments and projects that have challenged me to think critically, apply my knowledge in practical settings, and develop key problem-solving skills. These projects have spanned multiple disciplines, allowing me to gain hands-on experience, collaborate with peers, and refine my ability to research, analyse, and present findings effectively.\nFrom individual research papers to group-based projects, each assignment has contributed to my academic and professional development. Many of these tasks required not only technical expertise but also creativity, adaptability, and strong communication skills, qualities that will be invaluable as I move forward in my career.\nBelow, you‚Äôll find a collection of the assignments and projects I have completed during my final year in college. Feel free to explore them to gain insight into the work I have done and the skills I have developed along the way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Strategy\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding and Visualisation\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconometrics\n\n\n\n\n\n\n\n\n\n\n\n\n\nML and Advanced Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Enterprise Development\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/NED.html",
    "href": "projects/NED.html",
    "title": "New Enterprise Development",
    "section": "",
    "text": "Concept Paper\n\n\n\n\nFeasibilty Study\n\n\n\n\nMarketing Strategy\n\n\n\n\nMarketing Video\n\n\n\n\nBusiness Plan",
    "crumbs": [
      "New Enterprise Development"
    ]
  },
  {
    "objectID": "projects/business_strategy.html",
    "href": "projects/business_strategy.html",
    "title": "Business Strategy",
    "section": "",
    "text": "This module was looking at theorectical frameworks that are used in the job of management consultants. It was split into two seperate aspects, the first was a group project with a report worth 35% and a presentation on it worth 15%. While the second aspect was an indivdual case study assignment on Hitachi worth 50%.\nWhen it came to our group project we did this on Ornua, with a specfic focus on Kerrygold. Which we had an 80% on the report and a 78% on the presentation. You can see the presentation and the report below:\n\nPresentation\n\n\n\n\nReport\n\n\n\n\nCase Study\nThe Case study was a 3 week open book exam look at the strategic issue in Hitachi. You can see below:",
    "crumbs": [
      "Business Strategy"
    ]
  },
  {
    "objectID": "projects/sustainability.html",
    "href": "projects/sustainability.html",
    "title": "Sustainability",
    "section": "",
    "text": "Sustainability",
    "crumbs": [
      "Sustainability"
    ]
  },
  {
    "objectID": "projects/python/pythonvF.html",
    "href": "projects/python/pythonvF.html",
    "title": "Machine Learning Reflective Essay Code",
    "section": "",
    "text": "'''\nML Project for Module:\nBAA10127 - Data Analytics: Machine Learning & Advanced Python\nStudent No. 21311696\nStudent Name: Rory James Mulhern\nCourse: BSI4\nDataset: https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data\n'''\n\n'\\nML Project for Module:\\nBAA10127 - Data Analytics: Machine Learning & Advanced Python\\nStudent No. 21311696\\nStudent Name: Rory James Mulhern\\nCourse: BSI4\\nDataset: https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data\\n'\n\n\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import roc_curve, auc\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n\n# Linking file to Code\nfilepath = '/Users/mulhr/Desktop/ML Project/loan_data.csv'\n\n# Importing the dataset\nloans_df = pd.read_csv(filepath)\n\nloans_df\n\n\n\n\n\n\n\n\nperson_age\nperson_gender\nperson_education\nperson_income\nperson_emp_exp\nperson_home_ownership\nloan_amnt\nloan_intent\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\ncredit_score\nprevious_loan_defaults_on_file\nloan_status\n\n\n\n\n0\n22.0\nfemale\nMaster\n71948.0\n0\nRENT\n35000.0\nPERSONAL\n16.02\n0.49\n3.0\n561\nNo\n1\n\n\n1\n21.0\nfemale\nHigh School\n12282.0\n0\nOWN\n1000.0\nEDUCATION\n11.14\n0.08\n2.0\n504\nYes\n0\n\n\n2\n25.0\nfemale\nHigh School\n12438.0\n3\nMORTGAGE\n5500.0\nMEDICAL\n12.87\n0.44\n3.0\n635\nNo\n1\n\n\n3\n23.0\nfemale\nBachelor\n79753.0\n0\nRENT\n35000.0\nMEDICAL\n15.23\n0.44\n2.0\n675\nNo\n1\n\n\n4\n24.0\nmale\nMaster\n66135.0\n1\nRENT\n35000.0\nMEDICAL\n14.27\n0.53\n4.0\n586\nNo\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44995\n27.0\nmale\nAssociate\n47971.0\n6\nRENT\n15000.0\nMEDICAL\n15.66\n0.31\n3.0\n645\nNo\n1\n\n\n44996\n37.0\nfemale\nAssociate\n65800.0\n17\nRENT\n9000.0\nHOMEIMPROVEMENT\n14.07\n0.14\n11.0\n621\nNo\n1\n\n\n44997\n33.0\nmale\nAssociate\n56942.0\n7\nRENT\n2771.0\nDEBTCONSOLIDATION\n10.02\n0.05\n10.0\n668\nNo\n1\n\n\n44998\n29.0\nmale\nBachelor\n33164.0\n4\nRENT\n12000.0\nEDUCATION\n13.23\n0.36\n6.0\n604\nNo\n1\n\n\n44999\n24.0\nmale\nHigh School\n51609.0\n1\nRENT\n6665.0\nDEBTCONSOLIDATION\n17.05\n0.13\n3.0\n628\nNo\n1\n\n\n\n\n45000 rows √ó 14 columns\n\n\n\n\n# Youssef Elbadry Accessed: 9th April 2025\n\n# Looking at info on the data\nloans_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 45000 entries, 0 to 44999\nData columns (total 14 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   person_age                      45000 non-null  float64\n 1   person_gender                   45000 non-null  object \n 2   person_education                45000 non-null  object \n 3   person_income                   45000 non-null  float64\n 4   person_emp_exp                  45000 non-null  int64  \n 5   person_home_ownership           45000 non-null  object \n 6   loan_amnt                       45000 non-null  float64\n 7   loan_intent                     45000 non-null  object \n 8   loan_int_rate                   45000 non-null  float64\n 9   loan_percent_income             45000 non-null  float64\n 10  cb_person_cred_hist_length      45000 non-null  float64\n 11  credit_score                    45000 non-null  int64  \n 12  previous_loan_defaults_on_file  45000 non-null  object \n 13  loan_status                     45000 non-null  int64  \ndtypes: float64(6), int64(3), object(5)\nmemory usage: 4.8+ MB\n\n\n\n# Changing the person_age column to an integer\nloans_df['person_age'] = loans_df['person_age'].astype(int)\n\n# Looking at info on the data\nloans_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 45000 entries, 0 to 44999\nData columns (total 14 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   person_age                      45000 non-null  int64  \n 1   person_gender                   45000 non-null  object \n 2   person_education                45000 non-null  object \n 3   person_income                   45000 non-null  float64\n 4   person_emp_exp                  45000 non-null  int64  \n 5   person_home_ownership           45000 non-null  object \n 6   loan_amnt                       45000 non-null  float64\n 7   loan_intent                     45000 non-null  object \n 8   loan_int_rate                   45000 non-null  float64\n 9   loan_percent_income             45000 non-null  float64\n 10  cb_person_cred_hist_length      45000 non-null  float64\n 11  credit_score                    45000 non-null  int64  \n 12  previous_loan_defaults_on_file  45000 non-null  object \n 13  loan_status                     45000 non-null  int64  \ndtypes: float64(5), int64(4), object(5)\nmemory usage: 4.8+ MB\n\n\n\n# Removing duplicate rows\nloans_df.drop_duplicates(inplace=True)\n\n# Check if there are any duplicates left\nduplicate_count = loans_df.duplicated().sum()\n\n# Display final check\nif duplicate_count == 0:\n    print(\"No duplicate values in the dataset.\")\nelse:\n    print(f\"Total duplicate values remaining: {duplicate_count}\")\n\nNo duplicate values in the dataset.\n\n\n\n# Looking at the data description see the statistics of numeric columns\nloans_df.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nperson_age\n45000.0\n27.764178\n6.045108\n20.00\n24.00\n26.00\n30.00\n144.00\n\n\nperson_income\n45000.0\n80319.053222\n80422.498632\n8000.00\n47204.00\n67048.00\n95789.25\n7200766.00\n\n\nperson_emp_exp\n45000.0\n5.410333\n6.063532\n0.00\n1.00\n4.00\n8.00\n125.00\n\n\nloan_amnt\n45000.0\n9583.157556\n6314.886691\n500.00\n5000.00\n8000.00\n12237.25\n35000.00\n\n\nloan_int_rate\n45000.0\n11.006606\n2.978808\n5.42\n8.59\n11.01\n12.99\n20.00\n\n\nloan_percent_income\n45000.0\n0.139725\n0.087212\n0.00\n0.07\n0.12\n0.19\n0.66\n\n\ncb_person_cred_hist_length\n45000.0\n5.867489\n3.879702\n2.00\n3.00\n4.00\n8.00\n30.00\n\n\ncredit_score\n45000.0\n632.608756\n50.435865\n390.00\n601.00\n640.00\n670.00\n850.00\n\n\nloan_status\n45000.0\n0.222222\n0.415744\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\n\n\n\n\n\n\n# Youssef Elbadry Accessed: 9th April 2025\n\n# Seeing which columns are Categorical and Numerical\ncat_cols = [var for var in loans_df.columns if loans_df[var].dtypes == 'object']\nnum_cols = [var for var in loans_df.columns if loans_df[var].dtypes != 'object']\n\nprint(f'Categorical columns: {cat_cols}')\nprint(f'Numerical columns: {num_cols}')\n\nCategorical columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\nNumerical columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'loan_status']\n\n\n\ncat_cols\n\n['person_gender',\n 'person_education',\n 'person_home_ownership',\n 'loan_intent',\n 'previous_loan_defaults_on_file']\n\n\n\n# Seeing the split in gender\nloans_df['person_gender'].value_counts()\n\nperson_gender\nmale      24841\nfemale    20159\nName: count, dtype: int64\n\n\n\n# Youssef Elbadry Accessed: 9th April 2025\ndef plot_categorical_column(dataframe, column):\n\n    plt.figure(figsize=(7, 7))\n    ax = sns.countplot(x=dataframe[column])\n    total_count = len(dataframe[column])\n    threshold = 0.05 * total_count\n    category_counts = dataframe[column].value_counts(normalize=True) * 100\n    ax.axhline(threshold, color='red', linestyle='--', label=f'0.05% of total count ({threshold:.0f})')\n    \n    for p in ax.patches:\n        height = p.get_height()\n        percentage = (height / total_count) * 100\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * total_count, f'{percentage:.2f}%', ha=\"center\")\n    \n    plt.title(f'Label Cardinality for \"{column}\" Column')\n    plt.ylabel('Count')\n    plt.xlabel(column)\n    plt.tight_layout()\n    \n    plt.legend()\n    plt.show()\n\nfor col in cat_cols:\n    plot_categorical_column(loans_df, col)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloans_df[num_cols].hist(bins=30, figsize=(12,10))\nplt.show()\n\n\n\n\n\n\n\n\n\nlabel_prop = loans_df['loan_status'].value_counts()\n\nplt.pie(label_prop.values, labels=['Rejected (0)', 'Approved (1)'], autopct='%.2f')\nplt.title('Target label proportions')\nplt.show()\n\n\n\n\n\n\n\n\n\n'''\nArticle saying most lenders will not lend to anyone above 70\nhttps://www.moneysupermarket.com/loans/loans-for-pensioners/#:~:text=Most%20lenders%20have%20a%20maximum,beyond%20this%20age%20is%20rare.\n'''\nloans_df = loans_df[loans_df['person_age']&lt;= 70]\nprint('Ages above 70 removed!')\n\nAges above 70 removed!\n\n\n\nloans_df[num_cols].hist(bins=30, figsize=(12,10))\nplt.show()\n\n\n\n\n\n\n\n\n\n# Sulani Ishara Accessed: 14th April 2025\nnumerical_columns = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n\nfig, axes = plt.subplots(4, 2, figsize=(16, 20))\nfig.suptitle('Numerical Features vs Loan Status (Density Plots)', fontsize=16)\n\nfor i, col in enumerate(numerical_columns):\n    sns.kdeplot(data=loans_df, x=col, hue='loan_status', ax=axes[i//2, i%2], fill=True, common_norm=False, palette='muted')\n    axes[i//2, i%2].set_title(f'{col} vs Loan Status')\n    axes[i//2, i%2].set_xlabel(col)\n    axes[i//2, i%2].set_ylabel('Density')\n\nfig.delaxes(axes[3, 1])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Box and Whisker plot to see what the outliers in the dataset look like\n# Sulani Ishara Accessed: 14th April 2025\n\n# Function to perform univariate analysis for numeric columns\ndef univariate_analysis(data, column, title):\n    plt.figure(figsize=(10, 2))\n    \n    sns.boxplot(x=data[column], color='sandybrown')\n    plt.title(f'{title} Boxplot')\n    \n    plt.tight_layout()\n    plt.show()\n\n    print(f'\\nSummary Statistics for {title}:\\n', data[column].describe())\n\ncolumns_to_analyse = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n\nfor column in columns_to_analyse:\n    univariate_analysis(loans_df, column, column.replace('_', ' '))\n\n\n\n\n\n\n\n\n\nSummary Statistics for person age:\n count    44985.000000\nmean        27.739335\nstd          5.870099\nmin         20.000000\n25%         24.000000\n50%         26.000000\n75%         30.000000\nmax         70.000000\nName: person_age, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person income:\n count    4.498500e+04\nmean     7.991017e+04\nstd      6.332666e+04\nmin      8.000000e+03\n25%      4.719200e+04\n50%      6.704600e+04\n75%      9.578200e+04\nmax      2.448661e+06\nName: person_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person emp exp:\n count    44985.000000\nmean         5.385351\nstd          5.886303\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%          8.000000\nmax         50.000000\nName: person_emp_exp, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan amnt:\n count    44985.000000\nmean      9583.638368\nstd       6315.056351\nmin        500.000000\n25%       5000.000000\n50%       8000.000000\n75%      12238.000000\nmax      35000.000000\nName: loan_amnt, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan int rate:\n count    44985.000000\nmean        11.006678\nstd          2.979087\nmin          5.420000\n25%          8.590000\n50%         11.010000\n75%         12.990000\nmax         20.000000\nName: loan_int_rate, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan percent income:\n count    44985.000000\nmean         0.139743\nstd          0.087210\nmin          0.000000\n25%          0.070000\n50%          0.120000\n75%          0.190000\nmax          0.660000\nName: loan_percent_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for cb person cred hist length:\n count    44985.000000\nmean         5.863177\nstd          3.869127\nmin          2.000000\n25%          3.000000\n50%          4.000000\n75%          8.000000\nmax         30.000000\nName: cb_person_cred_hist_length, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for credit score:\n count    44985.000000\nmean       632.569123\nstd         50.388810\nmin        390.000000\n25%        601.000000\n50%        640.000000\n75%        670.000000\nmax        784.000000\nName: credit_score, dtype: float64\n\n\n\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy.stats.mstats import winsorize\n\nfor col in [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]:\n    loans_df[col] = winsorize(loans_df[col], limits=[0.025, 0.025])\n# Robust scaling\nscaler = RobustScaler()\nloans_df[[\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]] = scaler.fit_transform(loans_df[[\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]])\n\n# Box and Whisker plot to see what the outliers in the dataset look like\n# Function to perform univariate analysis for numeric columns\n\nfor column in columns_to_analyse:\n    univariate_analysis(loans_df, column, column.replace('_', ' '))\n\n/var/folders/h5/p6vdg3ps6wn1kgd_w454mf500000gn/T/ipykernel_28185/4078407632.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/h5/p6vdg3ps6wn1kgd_w454mf500000gn/T/ipykernel_28185/4078407632.py:8: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person age:\n count    44985.000000\nmean         0.265374\nstd          0.886138\nmin         -0.833333\n25%         -0.333333\n50%          0.000000\n75%          0.666667\nmax          2.833333\nName: person_age, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person income:\n count    44985.000000\nmean         0.207182\nstd          0.854803\nmin         -0.910846\n25%         -0.408603\n50%          0.000000\n75%          0.591397\nmax          2.888352\nName: person_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person emp exp:\n count    44985.000000\nmean         0.177688\nstd          0.763888\nmin         -0.571429\n25%         -0.428571\n50%          0.000000\n75%          0.571429\nmax          2.428571\nName: person_emp_exp, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan amnt:\n count    44985.000000\nmean         0.207831\nstd          0.833624\nmin         -0.898038\n25%         -0.414479\n50%          0.000000\n75%          0.585521\nmax          2.348715\nName: loan_amnt, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan int rate:\n count    44985.000000\nmean        11.006678\nstd          2.979087\nmin          5.420000\n25%          8.590000\n50%         11.010000\n75%         12.990000\nmax         20.000000\nName: loan_int_rate, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan percent income:\n count    44985.000000\nmean         0.139743\nstd          0.087210\nmin          0.000000\n25%          0.070000\n50%          0.120000\n75%          0.190000\nmax          0.660000\nName: loan_percent_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for cb person cred hist length:\n count    44985.000000\nmean         5.863177\nstd          3.869127\nmin          2.000000\n25%          3.000000\n50%          4.000000\n75%          8.000000\nmax         30.000000\nName: cb_person_cred_hist_length, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for credit score:\n count    44985.000000\nmean       632.569123\nstd         50.388810\nmin        390.000000\n25%        601.000000\n50%        640.000000\n75%        670.000000\nmax        784.000000\nName: credit_score, dtype: float64\n\n\n\ncolumns_to_check = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]\n\nfor col in columns_to_check:\n    skew_val = loans_df[col].skew()\n    print(f\"{col} skewness: {skew_val:.2f}\")\n\nperson_age skewness: 1.18\nperson_income skewness: 1.27\nperson_emp_exp skewness: 1.23\nloan_amnt skewness: 0.94\n\n\n\n# Apply log1p directly ‚Äî it's safe for 0s\nfor col in columns_to_check:\n    loans_df[col] = np.log1p(loans_df[col])\n\n# Recheck skewness\nfor col in columns_to_check:\n    skew_val = loans_df[col].skew()\n    print(f\"{col} skewness after log1p: {skew_val:.2f}\")\n\nfor column in columns_to_analyse:\n    univariate_analysis(loans_df, column, column.replace('_', ' '))\n\nperson_age skewness after log1p: -0.22\nperson_income skewness after log1p: -0.72\nperson_emp_exp skewness after log1p: 0.22\nloan_amnt skewness after log1p: -0.67\n\n\n/var/folders/h5/p6vdg3ps6wn1kgd_w454mf500000gn/T/ipykernel_28185/4222552184.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person age:\n count    44985.000000\nmean        -0.010294\nstd          0.726477\nmin         -1.791759\n25%         -0.405465\n50%          0.000000\n75%          0.510826\nmax          1.343735\nName: person_age, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person income:\n count    44985.000000\nmean        -0.084973\nstd          0.806123\nmin         -2.417388\n25%         -0.525267\n50%          0.000000\n75%          0.464613\nmax          1.357985\nName: person_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for person emp exp:\n count    44985.000000\nmean        -0.028691\nstd          0.616124\nmin         -0.847298\n25%         -0.559616\n50%          0.000000\n75%          0.451985\nmax          1.232144\nName: person_emp_exp, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan amnt:\n count    44985.000000\nmean        -0.089653\nstd          0.816802\nmin         -2.283156\n25%         -0.535253\n50%          0.000000\n75%          0.460913\nmax          1.208577\nName: loan_amnt, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan int rate:\n count    44985.000000\nmean        11.006678\nstd          2.979087\nmin          5.420000\n25%          8.590000\n50%         11.010000\n75%         12.990000\nmax         20.000000\nName: loan_int_rate, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for loan percent income:\n count    44985.000000\nmean         0.139743\nstd          0.087210\nmin          0.000000\n25%          0.070000\n50%          0.120000\n75%          0.190000\nmax          0.660000\nName: loan_percent_income, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for cb person cred hist length:\n count    44985.000000\nmean         5.863177\nstd          3.869127\nmin          2.000000\n25%          3.000000\n50%          4.000000\n75%          8.000000\nmax         30.000000\nName: cb_person_cred_hist_length, dtype: float64\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for credit score:\n count    44985.000000\nmean       632.569123\nstd         50.388810\nmin        390.000000\n25%        601.000000\n50%        640.000000\n75%        670.000000\nmax        784.000000\nName: credit_score, dtype: float64\n\n\n\nloans_df\nloans_df.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nperson_age\n44985.0\n-0.010294\n0.726477\n-1.791759\n-0.405465\n0.00\n0.510826\n1.343735\n\n\nperson_income\n44985.0\n-0.084973\n0.806123\n-2.417388\n-0.525267\n0.00\n0.464613\n1.357985\n\n\nperson_emp_exp\n44985.0\n-0.028691\n0.616124\n-0.847298\n-0.559616\n0.00\n0.451985\n1.232144\n\n\nloan_amnt\n44985.0\n-0.089653\n0.816802\n-2.283156\n-0.535253\n0.00\n0.460913\n1.208577\n\n\nloan_int_rate\n44985.0\n11.006678\n2.979087\n5.420000\n8.590000\n11.01\n12.990000\n20.000000\n\n\nloan_percent_income\n44985.0\n0.139743\n0.087210\n0.000000\n0.070000\n0.12\n0.190000\n0.660000\n\n\ncb_person_cred_hist_length\n44985.0\n5.863177\n3.869127\n2.000000\n3.000000\n4.00\n8.000000\n30.000000\n\n\ncredit_score\n44985.0\n632.569123\n50.388810\n390.000000\n601.000000\n640.00\n670.000000\n784.000000\n\n\nloan_status\n44985.0\n0.222296\n0.415794\n0.000000\n0.000000\n0.00\n0.000000\n1.000000\n\n\n\n\n\n\n\n\n# Sulani Ishara Accessed: 14th April 2025\nnumerical_columns = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n\nfig, axes = plt.subplots(4, 2, figsize=(16, 20))\nfig.suptitle('Numerical Features vs Loan Status (Density Plots)', fontsize=16)\n\nfor i, col in enumerate(numerical_columns):\n    sns.kdeplot(data=loans_df, x=col, hue='loan_status', ax=axes[i//2, i%2], fill=True, common_norm=False, palette='muted')\n    axes[i//2, i%2].set_title(f'{col} vs Loan Status')\n    axes[i//2, i%2].set_xlabel(col)\n    axes[i//2, i%2].set_ylabel('Density')\n\nfig.delaxes(axes[3, 1])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Making Education into a non-categorical columns\nloans_df['person_education'] = loans_df['person_education'].replace({\n    'High School': 0,\n    'Associate': 1,\n    'Bachelor': 2,\n    'Master': 3,\n    'Doctorate': 4\n})\n\n/var/folders/h5/p6vdg3ps6wn1kgd_w454mf500000gn/T/ipykernel_28185/1636345205.py:2: FutureWarning:\n\nDowncasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n\n/var/folders/h5/p6vdg3ps6wn1kgd_w454mf500000gn/T/ipykernel_28185/1636345205.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\nloans_df\n\n\n\n\n\n\n\n\nperson_age\nperson_gender\nperson_education\nperson_income\nperson_emp_exp\nperson_home_ownership\nloan_amnt\nloan_intent\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\ncredit_score\nprevious_loan_defaults_on_file\nloan_status\n\n\n\n\n0\n-1.098612\nfemale\n3\n0.096114\n-0.847298\nRENT\n1.208577\nPERSONAL\n16.02\n0.49\n3.0\n561\nNo\n1\n\n\n1\n-1.791759\nfemale\n0\n-2.417388\n-0.847298\nOWN\n-2.283156\nEDUCATION\n11.14\n0.08\n2.0\n504\nYes\n0\n\n\n2\n-0.182322\nfemale\n0\n-2.417388\n-0.154151\nMORTGAGE\n-0.423730\nMEDICAL\n12.87\n0.44\n3.0\n635\nNo\n1\n\n\n3\n-0.693147\nfemale\n2\n0.232313\n-0.847298\nRENT\n1.208577\nMEDICAL\n15.23\n0.44\n2.0\n675\nNo\n1\n\n\n4\n-0.405465\nmale\n3\n-0.018927\n-0.559616\nRENT\n1.208577\nMEDICAL\n14.27\n0.53\n4.0\n586\nNo\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44995\n0.154151\nmale\n1\n-0.498519\n0.251314\nRENT\n0.676570\nMEDICAL\n15.66\n0.31\n3.0\n645\nNo\n1\n\n\n44996\n1.041454\nfemale\n1\n-0.025978\n1.049822\nRENT\n0.129413\nHOMEIMPROVEMENT\n14.07\n0.14\n11.0\n621\nNo\n1\n\n\n44997\n0.773190\nmale\n1\n-0.233123\n0.356675\nRENT\n-1.281708\nDEBTCONSOLIDATION\n10.02\n0.05\n10.0\n668\nNo\n1\n\n\n44998\n0.405465\nmale\n2\n-1.195026\n0.000000\nRENT\n0.439956\nEDUCATION\n13.23\n0.36\n6.0\n604\nNo\n1\n\n\n44999\n-0.405465\nmale\n0\n-0.382285\n-0.559616\nRENT\n-0.203884\nDEBTCONSOLIDATION\n17.05\n0.13\n3.0\n628\nNo\n1\n\n\n\n\n44985 rows √ó 14 columns\n\n\n\n\n# One-hot coding for dummy variables\nloans_df = pd.get_dummies(loans_df, columns = ['person_gender', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file'], drop_first = True)\n\n# Checking the data types\nloans_df.dtypes\n\nperson_age                            float64\nperson_education                        int64\nperson_income                         float64\nperson_emp_exp                        float64\nloan_amnt                             float64\nloan_int_rate                         float64\nloan_percent_income                   float64\ncb_person_cred_hist_length            float64\ncredit_score                            int64\nloan_status                             int64\nperson_gender_male                       bool\nperson_home_ownership_OTHER              bool\nperson_home_ownership_OWN                bool\nperson_home_ownership_RENT               bool\nloan_intent_EDUCATION                    bool\nloan_intent_HOMEIMPROVEMENT              bool\nloan_intent_MEDICAL                      bool\nloan_intent_PERSONAL                     bool\nloan_intent_VENTURE                      bool\nprevious_loan_defaults_on_file_Yes       bool\ndtype: object\n\n\n\n# Define numerical columns with target\nnumerical_columns_with_target = [\n    'person_age', \n    'person_income', \n    'person_emp_exp', \n    'loan_amnt', \n    'loan_int_rate', \n    'loan_percent_income', \n    'cb_person_cred_hist_length', \n    'credit_score'\n]\n\n# Create pairplot of numerical features with loan_status as hue\nsns.pairplot(loans_df[numerical_columns_with_target + ['loan_status']], \n             hue='loan_status', \n             palette='muted'\n            )\nplt.show()\n\n\n\n\n\n\n\n\n\n# Getting a correlation matrix\nnum_loans_df = loans_df.select_dtypes(include=['number']) # Include only numerical data types\n\n# Correlation of that data\ncorr_matrix = num_loans_df.corr()\nprint(corr_matrix)\n\n                            person_age  person_education  person_income  \\\nperson_age                    1.000000          0.035695       0.155206   \nperson_education              0.035695          1.000000       0.010170   \nperson_income                 0.155206          0.010170       1.000000   \nperson_emp_exp                0.872665          0.025813       0.115596   \nloan_amnt                     0.067378          0.001772       0.412755   \nloan_int_rate                 0.014931          0.003674      -0.027007   \nloan_percent_income          -0.054326         -0.004378      -0.348179   \ncb_person_cred_hist_length    0.769746         -0.004589       0.082971   \ncredit_score                  0.157042          0.211911       0.024444   \nloan_status                  -0.033822         -0.001160      -0.305005   \n\n                            person_emp_exp  loan_amnt  loan_int_rate  \\\nperson_age                        0.872665   0.067378       0.014931   \nperson_education                  0.025813   0.001772       0.003674   \nperson_income                     0.115596   0.412755      -0.027007   \nperson_emp_exp                    1.000000   0.048703       0.019044   \nloan_amnt                         0.048703   1.000000       0.095544   \nloan_int_rate                     0.019044   0.095544       1.000000   \nloan_percent_income              -0.045281   0.611334       0.125301   \ncb_person_cred_hist_length        0.763690   0.035027       0.018332   \ncredit_score                      0.172743   0.007027       0.011539   \nloan_status                      -0.026595   0.075708       0.332032   \n\n                            loan_percent_income  cb_person_cred_hist_length  \\\nperson_age                            -0.054326                    0.769746   \nperson_education                      -0.004378                   -0.004589   \nperson_income                         -0.348179                    0.082971   \nperson_emp_exp                        -0.045281                    0.763690   \nloan_amnt                              0.611334                    0.035027   \nloan_int_rate                          0.125301                    0.018332   \nloan_percent_income                    1.000000                   -0.031191   \ncb_person_cred_hist_length            -0.031191                    1.000000   \ncredit_score                          -0.010976                    0.153466   \nloan_status                            0.384864                   -0.014299   \n\n                            credit_score  loan_status  \nperson_age                      0.157042    -0.033822  \nperson_education                0.211911    -0.001160  \nperson_income                   0.024444    -0.305005  \nperson_emp_exp                  0.172743    -0.026595  \nloan_amnt                       0.007027     0.075708  \nloan_int_rate                   0.011539     0.332032  \nloan_percent_income            -0.010976     0.384864  \ncb_person_cred_hist_length      0.153466    -0.014299  \ncredit_score                    1.000000    -0.007235  \nloan_status                    -0.007235     1.000000  \n\n\n\n# Visual the Correlation Matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Matrix of Variables')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Drop Person Employment Experience and Age\nloans_df = loans_df.drop(columns=['person_emp_exp','person_age'])\nloans_df\n\n\n\n\n\n\n\n\nperson_education\nperson_income\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\ncredit_score\nloan_status\nperson_gender_male\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprevious_loan_defaults_on_file_Yes\n\n\n\n\n0\n3\n0.096114\n1.208577\n16.02\n0.49\n3.0\n561\n1\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\n0\n-2.417388\n-2.283156\n11.14\n0.08\n2.0\n504\n0\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n0\n-2.417388\n-0.423730\n12.87\n0.44\n3.0\n635\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\n2\n0.232313\n1.208577\n15.23\n0.44\n2.0\n675\n1\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n3\n-0.018927\n1.208577\n14.27\n0.53\n4.0\n586\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44995\n1\n-0.498519\n0.676570\n15.66\n0.31\n3.0\n645\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n44996\n1\n-0.025978\n0.129413\n14.07\n0.14\n11.0\n621\n1\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n44997\n1\n-0.233123\n-1.281708\n10.02\n0.05\n10.0\n668\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n44998\n2\n-1.195026\n0.439956\n13.23\n0.36\n6.0\n604\n1\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n44999\n0\n-0.382285\n-0.203884\n17.05\n0.13\n3.0\n628\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n44985 rows √ó 18 columns\n\n\n\n\n# Create a new column for custom labels\nloans_df['loan_status_label'] = loans_df['loan_status'].map({0: 'Denied (0)', 1: 'Approved (1)'})\n\n# Create a histogram plotting Approved and Denied loans\nsns.histplot(\n    data=loans_df,\n    x='loan_status_label',\n    hue='loan_status_label',\n    palette={\"Denied (0)\": \"red\", \"Approved (1)\": \"green\"}\n)\nplt.title(\"Amount of Denied and Approved Loans\")\nplt.xlabel(\"Loan Status\")\nplt.ylabel(\"Count\")\nplt.show\n\n\n\n\n\n\n\n\n\n# Splitting the Dataset into X and Y\nX = loans_df.drop(columns=['loan_status', 'loan_status_label'])  \ny = loans_df['loan_status'] \n\n\nX\n\n\n\n\n\n\n\n\nperson_education\nperson_income\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\ncredit_score\nperson_gender_male\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprevious_loan_defaults_on_file_Yes\n\n\n\n\n0\n3\n0.096114\n1.208577\n16.02\n0.49\n3.0\n561\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\n0\n-2.417388\n-2.283156\n11.14\n0.08\n2.0\n504\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n0\n-2.417388\n-0.423730\n12.87\n0.44\n3.0\n635\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\n2\n0.232313\n1.208577\n15.23\n0.44\n2.0\n675\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n3\n-0.018927\n1.208577\n14.27\n0.53\n4.0\n586\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44995\n1\n-0.498519\n0.676570\n15.66\n0.31\n3.0\n645\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n44996\n1\n-0.025978\n0.129413\n14.07\n0.14\n11.0\n621\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n44997\n1\n-0.233123\n-1.281708\n10.02\n0.05\n10.0\n668\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n44998\n2\n-1.195026\n0.439956\n13.23\n0.36\n6.0\n604\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n44999\n0\n-0.382285\n-0.203884\n17.05\n0.13\n3.0\n628\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n44985 rows √ó 17 columns\n\n\n\n\ny\n\n0        1\n1        0\n2        1\n3        1\n4        1\n        ..\n44995    1\n44996    1\n44997    1\n44998    1\n44999    1\nName: loan_status, Length: 44985, dtype: int64\n\n\n\n# Splitting the dataset into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nlr_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    reg_model_lr = LogisticRegression(max_iter=200000, random_state=42)\n    reg_model_lr.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    lr_accuracy = reg_model_lr.score(X_test, y_test)\n    lr_accuracy_scores.append(lr_accuracy)\n    print(f\"Fold {fold} Accuracy: {lr_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9000\nFold 2 Accuracy: 0.9046\nFold 3 Accuracy: 0.8998\nFold 4 Accuracy: 0.8991\nFold 5 Accuracy: 0.8993\nFold 6 Accuracy: 0.8993\nFold 7 Accuracy: 0.8993\nFold 8 Accuracy: 0.8962\nFold 9 Accuracy: 0.8980\nFold 10 Accuracy: 0.8986\nAverage Accuracy: 0.8994\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nlr2_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    reg_model_lr2 = LogisticRegression(max_iter=200000, random_state=42, penalty='l2')\n    reg_model_lr2.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    lr2_accuracy = reg_model_lr2.score(X_test, y_test)\n    lr2_accuracy_scores.append(lr2_accuracy)\n    print(f\"Fold {fold} Accuracy: {lr2_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9000\nFold 2 Accuracy: 0.9046\nFold 3 Accuracy: 0.8998\nFold 4 Accuracy: 0.8991\nFold 5 Accuracy: 0.8993\nFold 6 Accuracy: 0.8993\nFold 7 Accuracy: 0.8993\nFold 8 Accuracy: 0.8962\nFold 9 Accuracy: 0.8980\nFold 10 Accuracy: 0.8986\nAverage Accuracy: 0.8994\n\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_lr = reg_model_lr.predict(X_test)\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_lr2 = reg_model_lr2.predict(X_test)\n\n\n# Compute the evaluation metrics\nlr_precision = precision_score(y_test, predictions_lr)\nlr_recall = recall_score (y_test, predictions_lr)\nlr_f1 = f1_score(y_test, predictions_lr)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")\nprint(f\"Precision: {lr_precision:.4f}\")\nprint(f\"Recall: {lr_recall:.4f}\")\nprint(f\"F1-Score: {lr_f1:.4f}\")\n\nAverage Accuracy: 0.8994\nPrecision: 0.7804\nRecall: 0.7570\nF1-Score: 0.7685\n\n\n\n# Compute the evaluation metrics\nlr2_precision = precision_score(y_test, predictions_lr2)\nlr2_recall = recall_score (y_test, predictions_lr2)\nlr2_f1 = f1_score(y_test, predictions_lr2)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")\nprint(f\"Precision: {lr2_precision:.4f}\")\nprint(f\"Recall: {lr2_recall:.4f}\")\nprint(f\"F1-Score: {lr2_f1:.4f}\")\n\nAverage Accuracy: 0.8994\nPrecision: 0.7804\nRecall: 0.7570\nF1-Score: 0.7685\n\n\n\nlr_cm = confusion_matrix(y_test, predictions_lr )\nprint(lr_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(lr_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (Logistic Regression)\", fontsize=14)\nplt.show()\n\n[[3285  213]\n [ 243  757]]\n\n\n\n\n\n\n\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nlr_y_prob = reg_model_lr.predict_proba(X_test)[:, 1]\n\nlr_auc_roc = roc_auc_score(y_test, lr_y_prob)\nprint(f\"AUC-ROC: {lr_auc_roc:.4f}\")\n\nAUC-ROC: 0.9552\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nlr2_y_prob = reg_model_lr2.predict_proba(X_test)[:, 1]\n\nlr2_auc_roc = roc_auc_score(y_test, lr2_y_prob)\nprint(f\"AUC-ROC: {lr2_auc_roc:.4f}\")\n\nAUC-ROC: 0.9552\n\n\n\n# From ChatGPT\n\n# Get false positive rate, true positive rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, lr_y_prob)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {lr_auc_roc:.4f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve (Logistic Regression)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\ndt_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    dt_model = DecisionTreeClassifier(random_state=42)\n    dt_model.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    dt_accuracy = dt_model.score(X_test, y_test)\n    dt_accuracy_scores.append(dt_accuracy)\n    print(f\"Fold {fold} Accuracy: {dt_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9029\nFold 2 Accuracy: 0.9111\nFold 3 Accuracy: 0.9089\nFold 4 Accuracy: 0.8929\nFold 5 Accuracy: 0.9004\nFold 6 Accuracy: 0.8991\nFold 7 Accuracy: 0.9024\nFold 8 Accuracy: 0.9020\nFold 9 Accuracy: 0.8984\nFold 10 Accuracy: 0.8986\nAverage Accuracy: 0.9017\n\n\n\n# Getting the predictions for the Decision Tree Model\npredictions_dt = dt_model.predict(X_test)\n\n\n# Compute the evaluation metrics\ndt_precision = precision_score(y_test, predictions_dt)\ndt_recall = recall_score (y_test, predictions_dt)\ndt_f1 = f1_score(y_test, predictions_dt)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")\nprint(f\"Precision: {dt_precision:.4f}\")\nprint(f\"Recall: {dt_recall:.4f}\")\nprint(f\"F1-Score: {dt_f1:.4f}\")\n\nAverage Accuracy: 0.9017\nPrecision: 0.7677\nRecall: 0.7800\nF1-Score: 0.7738\n\n\n\ndt_cm = confusion_matrix(y_test, predictions_dt )\nprint(dt_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(dt_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (Decision Tree)\", fontsize=14)\nplt.show()\n\n[[3262  236]\n [ 220  780]]\n\n\n\n\n\n\n\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\ndt_y_prob = dt_model.predict_proba(X_test)[:, 1]\n\ndt_auc_roc = roc_auc_score(y_test, dt_y_prob)\nprint(f\"AUC-ROC: {dt_auc_roc:.4f}\")\n\nAUC-ROC: 0.8563\n\n\n\n# From ChatGPT\n\n# Get false positive rate, true positive rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, dt_y_prob)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {dt_auc_roc:.4f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve (Decsision Tree)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nrf_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_model.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    rf_accuracy = rf_model.score(X_test, y_test)\n    rf_accuracy_scores.append(rf_accuracy)\n    print(f\"Fold {fold} Accuracy: {rf_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9249\nFold 2 Accuracy: 0.9349\nFold 3 Accuracy: 0.9307\nFold 4 Accuracy: 0.9275\nFold 5 Accuracy: 0.9280\nFold 6 Accuracy: 0.9340\nFold 7 Accuracy: 0.9311\nFold 8 Accuracy: 0.9293\nFold 9 Accuracy: 0.9273\nFold 10 Accuracy: 0.9289\nAverage Accuracy: 0.9296\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nrf2_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    rf2_model = RandomForestClassifier(n_estimators=200, \n                                       random_state=42, \n                                       max_depth=8,\n                                       min_samples_split=5,\n                                       min_samples_leaf=2,\n                                       max_features='sqrt',\n                                       bootstrap=True)\n    rf2_model.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    rf2_accuracy = rf2_model.score(X_test, y_test)\n    rf2_accuracy_scores.append(rf2_accuracy)\n    print(f\"Fold {fold} Accuracy: {rf2_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9224\nFold 2 Accuracy: 0.9262\nFold 3 Accuracy: 0.9193\nFold 4 Accuracy: 0.9193\nFold 5 Accuracy: 0.9206\nFold 6 Accuracy: 0.9231\nFold 7 Accuracy: 0.9173\nFold 8 Accuracy: 0.9202\nFold 9 Accuracy: 0.9215\nFold 10 Accuracy: 0.9240\nAverage Accuracy: 0.9214\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nrf3_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    rf3_model = RandomForestClassifier(n_estimators=200, \n                                       random_state=42, \n                                       max_depth=8,\n                                       min_samples_split=5,\n                                       min_samples_leaf=2,\n                                       max_features='sqrt',\n                                       bootstrap=False)\n    rf3_model.fit(X_resampled, y_resampled)\n    \n    # Evaluate the model on the test data\n    rf3_accuracy = rf3_model.score(X_test, y_test)\n    rf3_accuracy_scores.append(rf3_accuracy)\n    print(f\"Fold {fold} Accuracy: {rf3_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9204\nFold 2 Accuracy: 0.9255\nFold 3 Accuracy: 0.9193\nFold 4 Accuracy: 0.9182\nFold 5 Accuracy: 0.9213\nFold 6 Accuracy: 0.9235\nFold 7 Accuracy: 0.9175\nFold 8 Accuracy: 0.9222\nFold 9 Accuracy: 0.9220\nFold 10 Accuracy: 0.9240\nAverage Accuracy: 0.9214\n\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_rf = rf_model.predict(X_test)\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_rf2 = rf2_model.predict(X_test)\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_rf3 = rf3_model.predict(X_test)\n\n\n# Compute the evaluation metrics\nrf_precision = precision_score(y_test, predictions_rf)\nrf_recall = recall_score (y_test, predictions_rf)\nrf_f1 = f1_score(y_test, predictions_rf)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf_precision:.4f}\")\nprint(f\"Recall: {rf_recall:.4f}\")\nprint(f\"F1-Score: {rf_f1:.4f}\")\n\nAverage Accuracy: 0.9296\nPrecision: 0.8972\nRecall: 0.7680\nF1-Score: 0.8276\n\n\n\n# Compute the evaluation metrics\nrf2_precision = precision_score(y_test, predictions_rf2)\nrf2_recall = recall_score (y_test, predictions_rf2)\nrf2_f1 = f1_score(y_test, predictions_rf2)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf2_precision:.4f}\")\nprint(f\"Recall: {rf2_recall:.4f}\")\nprint(f\"F1-Score: {rf2_f1:.4f}\")\n\nAverage Accuracy: 0.9214\nPrecision: 0.9175\nRecall: 0.7230\nF1-Score: 0.8087\n\n\n\n# Compute the evaluation metrics\nrf3_precision = precision_score(y_test, predictions_rf3)\nrf3_recall = recall_score (y_test, predictions_rf3)\nrf3_f1 = f1_score(y_test, predictions_rf3)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf3_precision:.4f}\")\nprint(f\"Recall: {rf3_recall:.4f}\")\nprint(f\"F1-Score: {rf3_f1:.4f}\")\n\nAverage Accuracy: 0.9214\nPrecision: 0.9218\nRecall: 0.7190\nF1-Score: 0.8079\n\n\n\nrf_cm = confusion_matrix(y_test, predictions_rf)\nprint(rf_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (Random Forest (Untuned))\", fontsize=14)\nplt.show()\n\n[[3410   88]\n [ 232  768]]\n\n\n\n\n\n\n\n\n\n\nrf2_cm = confusion_matrix(y_test, predictions_rf2)\nprint(rf2_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(rf2_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (Random Forest (Tuned v1))\", fontsize=14)\nplt.show()\n\n[[3433   65]\n [ 277  723]]\n\n\n\n\n\n\n\n\n\n\nrf3_cm = confusion_matrix(y_test, predictions_rf2)\nprint(rf3_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(rf3_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (Random Forest (Tuned v2))\", fontsize=14)\nplt.show()\n\n[[3433   65]\n [ 277  723]]\n\n\n\n\n\n\n\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nrf_y_prob = rf_model.predict_proba(X_test)[:, 1]\n\nrf_auc_roc = roc_auc_score(y_test, rf_y_prob)\nprint(f\"AUC-ROC: {rf_auc_roc:.4f}\")\n\nAUC-ROC: 0.9747\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nrf2_y_prob = rf2_model.predict_proba(X_test)[:, 1]\n\nrf2_auc_roc = roc_auc_score(y_test, rf2_y_prob)\nprint(f\"AUC-ROC: {rf2_auc_roc:.4f}\")\n\nAUC-ROC: 0.9683\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nrf3_y_prob = rf3_model.predict_proba(X_test)[:, 1]\n\nrf3_auc_roc = roc_auc_score(y_test, rf3_y_prob)\nprint(f\"AUC-ROC: {rf3_auc_roc:.4f}\")\n\nAUC-ROC: 0.9684\n\n\n\n# From ChatGPT\n\n# Get false positive rate, true positive rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, rf_y_prob)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {rf_auc_roc:.4f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve (Random Forest (Untuned))')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Dictionary of model names and predicted probabilities\nmodels_probs = {\n    \"Random Forest(Untuned)\": rf_y_prob,\n    \"Random Forest(Tuned v1)\": rf2_y_prob,\n    \"Random Forest(Tuned v2)\": rf3_y_prob\n}\n\nplt.figure(figsize=(10, 8))\n\n# Plot each ROC curve\nfor name, probs in models_probs.items():\n    fpr, tpr, _ = roc_curve(y_test, probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.5f})')\n\n# Plot random guess line\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison of Models (Random Forest)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nxgb_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    xgb_model = XGBClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        eval_metric='logloss',\n        random_state=42\n    )\n    xgb_model.fit(X_train, y_train)\n    \n    # Evaluate the model on the test data\n    xgb_accuracy = xgb_model.score(X_test, y_test)\n    xgb_accuracy_scores.append(xgb_accuracy)\n    print(f\"Fold {fold} Accuracy: {xgb_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9355\nFold 2 Accuracy: 0.9413\nFold 3 Accuracy: 0.9382\nFold 4 Accuracy: 0.9378\nFold 5 Accuracy: 0.9353\nFold 6 Accuracy: 0.9426\nFold 7 Accuracy: 0.9360\nFold 8 Accuracy: 0.9360\nFold 9 Accuracy: 0.9373\nFold 10 Accuracy: 0.9353\nAverage Accuracy: 0.9375\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nxgb2_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    xgb2_model = XGBClassifier(\n        n_estimators=100,\n        max_depth=8,\n        learning_rate=0.1,\n        eval_metric='logloss',\n        random_state=42\n    )\n    xgb2_model.fit(X_train, y_train)\n    \n    # Evaluate the model on the test data\n    xgb2_accuracy = xgb2_model.score(X_test, y_test)\n    xgb2_accuracy_scores.append(xgb2_accuracy)\n    print(f\"Fold {fold} Accuracy: {xgb2_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.9444\nFold 2 Accuracy: 0.9529\nFold 3 Accuracy: 0.9498\nFold 4 Accuracy: 0.9482\nFold 5 Accuracy: 0.9495\nFold 6 Accuracy: 0.9531\nFold 7 Accuracy: 0.9480\nFold 8 Accuracy: 0.9453\nFold 9 Accuracy: 0.9471\nFold 10 Accuracy: 0.9478\nAverage Accuracy: 0.9486\n\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_xgb = xgb_model.predict(X_test)\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_xgb2 = xgb2_model.predict(X_test)\n\n\n# Compute the evaluation metrics\nxgb_precision = precision_score(y_test, predictions_xgb)\nxgb_recall = recall_score (y_test, predictions_xgb)\nxgb_f1 = f1_score(y_test, predictions_xgb)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\nprint(f\"Precision: {xgb_precision:.4f}\")\nprint(f\"Recall: {xgb_recall:.4f}\")\nprint(f\"F1-Score: {xgb_f1:.4f}\")\n\nAverage Accuracy: 0.9375\nPrecision: 0.9098\nRecall: 0.7870\nF1-Score: 0.8440\n\n\n\n# Compute the evaluation metrics\nxgb2_precision = precision_score(y_test, predictions_xgb2)\nxgb2_recall = recall_score (y_test, predictions_xgb2)\nxgb2_f1 = f1_score(y_test, predictions_xgb2)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")\nprint(f\"Precision: {xgb2_precision:.4f}\")\nprint(f\"Recall: {xgb2_recall:.4f}\")\nprint(f\"F1-Score: {xgb2_f1:.4f}\")\n\nAverage Accuracy: 0.9486\nPrecision: 0.9332\nRecall: 0.8240\nF1-Score: 0.8752\n\n\n\nxgb_cm = confusion_matrix(y_test, predictions_xgb)\nprint(xgb_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(xgb_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (XGBoost (Untuned))\", fontsize=14)\nplt.show()\n\n[[3420   78]\n [ 213  787]]\n\n\n\n\n\n\n\n\n\n\nxgb2_cm = confusion_matrix(y_test, predictions_xgb2)\nprint(xgb2_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(xgb2_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (XGBoost (Tuned))\", fontsize=14)\nplt.show()\n\n[[3439   59]\n [ 176  824]]\n\n\n\n\n\n\n\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nxgb_y_prob = xgb_model.predict_proba(X_test)[:, 1]\n\nxgb_auc_roc = roc_auc_score(y_test, xgb_y_prob)\nprint(f\"AUC-ROC: {xgb_auc_roc:.4f}\")\n\nAUC-ROC: 0.9810\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nxgb2_y_prob = xgb2_model.predict_proba(X_test)[:, 1]\n\nxgb2_auc_roc = roc_auc_score(y_test, xgb2_y_prob)\nprint(f\"AUC-ROC: {xgb2_auc_roc:.4f}\")\n\nAUC-ROC: 0.9868\n\n\n\n# From ChatGPT\n\n# Get false positive rate, true positive rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, xgb_y_prob)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {xgb_auc_roc:.4f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve (XGBoost (Untuned))')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Dictionary of model names and predicted probabilities\nmodels_probs = {\n    \"XGBoost (Tuned)\": xgb2_y_prob,\n    \"XGBoost (Untuned)\": xgb_y_prob,\n}\n\nplt.figure(figsize=(10, 8))\n\n# Plot each ROC curve\nfor name, probs in models_probs.items():\n    fpr, tpr, _ = roc_curve(y_test, probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.5f})')\n\n# Plot random guess line\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison of Models (XGBoost)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Setting Up 10-Fold Stratified Cross-Validation\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nknn_accuracy_scores = []\n\n# Loop through each fold\nfor fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # --- Model Training ---\n    knn_model = KNeighborsClassifier(\n        n_neighbors=2, \n        weights='uniform', \n        algorithm='auto', \n        leaf_size=30, \n        metric='minkowski'\n    )\n    knn_model.fit(X_train, y_train)\n    \n    # Evaluate the model on the test data\n    knn_accuracy = knn_model.score(X_test, y_test)\n    knn_accuracy_scores.append(knn_accuracy)\n    print(f\"Fold {fold} Accuracy: {knn_accuracy:.4f}\")\n    \nprint(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(knn_accuracy_scores):.4f}\")\n\nFold 1 Accuracy: 0.8735\nFold 2 Accuracy: 0.8629\nFold 3 Accuracy: 0.8702\nFold 4 Accuracy: 0.8675\nFold 5 Accuracy: 0.8649\nFold 6 Accuracy: 0.8722\nFold 7 Accuracy: 0.8688\nFold 8 Accuracy: 0.8684\nFold 9 Accuracy: 0.8686\nFold 10 Accuracy: 0.8610\nAverage Accuracy: 0.8678\n\n\n\n# Getting the predictions for the Logistic Regression Model\npredictions_knn = knn_model.predict(X_test)\n\n\n# Compute the evaluation metrics\nknn_precision = precision_score(y_test, predictions_knn)\nknn_recall = recall_score (y_test, predictions_knn)\nknn_f1 = f1_score(y_test, predictions_knn)\n\n# Print out evaluation metrics\nprint(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\nprint(f\"Precision: {knn_precision:.4f}\")\nprint(f\"Recall: {knn_recall:.4f}\")\nprint(f\"F1-Score: {knn_f1:.4f}\")\n\nAverage Accuracy: 0.8678\nPrecision: 0.8947\nRecall: 0.4250\nF1-Score: 0.5763\n\n\n\nknn_cm = confusion_matrix(y_test, predictions_knn)\nprint(knn_cm)\n\n# Define new labels: index 0 -&gt; \"Denied\", index 1 -&gt; \"Approved\"\nlabels = ['Denied', 'Approved']\n\n# Plot the confusion matrix heatmap with the renamed labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(knn_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.title(\"Confusion Matrix (KNN)\", fontsize=14)\nplt.show()\n\n[[3448   50]\n [ 575  425]]\n\n\n\n\n\n\n\n\n\n\n# Calculating the AUC-ROC | from one of the tutorials\nknn_y_prob = knn_model.predict_proba(X_test)[:, 1]\n\nknn_auc_roc = roc_auc_score(y_test, knn_y_prob)\nprint(f\"AUC-ROC: {knn_auc_roc:.4f}\")\n\nAUC-ROC: 0.8882\n\n\n\n# Get false positive rate, true positive rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, knn_y_prob)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {knn_auc_roc:.4f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve (KNN)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Dictionary of model names and predicted probabilities\nmodels_probs = {\n    \"Logistic Regression\": lr_y_prob,\n    \"Decision Tree\": dt_y_prob,\n    \"Random Forest\": rf_y_prob,\n    \"Random Forest(Tuned v1)\": rf2_y_prob,\n    \"Random Forest(Tuned v2)\": rf3_y_prob,\n    \"XGBoost (Tuned)\": xgb2_y_prob,\n    \"XGBoost (Untuned)\": xgb_y_prob,\n    \"KNN\": knn_y_prob\n\n}\n\nplt.figure(figsize=(10, 8))\n\n# Plot each ROC curve\nfor name, probs in models_probs.items():\n    fpr, tpr, _ = roc_curve(y_test, probs)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n\n# Plot random guess line\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison of Models (All Models)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Print out all evaluation metrics\nprint(\"Logistic Regression (Untuned) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")\nprint(f\"Precision: {lr_precision:.4f}\")\nprint(f\"Recall: {lr_recall:.4f}\")\nprint(f\"F1-Score: {lr_f1:.4f}\")\nprint(f\"AUC-ROC: {lr_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"Logistic Regression (Tuned) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")\nprint(f\"Precision: {lr2_precision:.4f}\")\nprint(f\"Recall: {lr2_recall:.4f}\")\nprint(f\"F1-Score: {lr2_f1:.4f}\")\nprint(f\"AUC-ROC: {lr2_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"Decision Tree Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")\nprint(f\"Precision: {dt_precision:.4f}\")\nprint(f\"Recall: {dt_recall:.4f}\")\nprint(f\"F1-Score: {dt_f1:.4f}\")\nprint(f\"AUC-ROC: {dt_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"Random Forest (Untuned) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf_precision:.4f}\")\nprint(f\"Recall: {rf_recall:.4f}\")\nprint(f\"F1-Score: {rf_f1:.4f}\")\nprint(f\"AUC-ROC: {rf_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"Random Forest (Tuned v1) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf2_precision:.4f}\")\nprint(f\"Recall: {rf2_recall:.4f}\")\nprint(f\"F1-Score: {rf2_f1:.4f}\")\nprint(f\"AUC-ROC: {rf2_auc_roc:.4f}\")\n\nprint( \"  \")\nprint(\"Random Forest (Tuned v2) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")\nprint(f\"Precision: {rf3_precision:.4f}\")\nprint(f\"Recall: {rf3_recall:.4f}\")\nprint(f\"F1-Score: {rf3_f1:.4f}\")\nprint(f\"AUC-ROC: {rf3_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"XGBoost (Untuned) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\nprint(f\"Precision: {xgb_precision:.4f}\")\nprint(f\"Recall: {xgb_recall:.4f}\")\nprint(f\"F1-Score: {xgb_f1:.4f}\")\nprint(f\"AUC-ROC: {xgb_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"XGBoost (Tuned) Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")\nprint(f\"Precision: {xgb2_precision:.4f}\")\nprint(f\"Recall: {xgb2_recall:.4f}\")\nprint(f\"F1-Score: {xgb2_f1:.4f}\")\nprint(f\"AUC-ROC: {xgb2_auc_roc:.4f}\")\n\nprint(\"  \")\nprint(\"KNN Model Evaluation Metrics:\")\nprint(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\nprint(f\"Precision: {knn_precision:.4f}\")\nprint(f\"Recall: {knn_recall:.4f}\")\nprint(f\"F1-Score: {knn_f1:.4f}\")\nprint(f\"AUC-ROC: {knn_auc_roc:.4f}\")\n\nLogistic Regression (Untuned) Model Evaluation Metrics:\nAverage Accuracy: 0.8994\nPrecision: 0.7804\nRecall: 0.7570\nF1-Score: 0.7685\nAUC-ROC: 0.9552\n  \nLogistic Regression (Tuned) Model Evaluation Metrics:\nAverage Accuracy: 0.8994\nPrecision: 0.7804\nRecall: 0.7570\nF1-Score: 0.7685\nAUC-ROC: 0.9552\n  \nDecision Tree Model Evaluation Metrics:\nAverage Accuracy: 0.9017\nPrecision: 0.7677\nRecall: 0.7800\nF1-Score: 0.7738\nAUC-ROC: 0.8563\n  \nRandom Forest (Untuned) Model Evaluation Metrics:\nAverage Accuracy: 0.9296\nPrecision: 0.8972\nRecall: 0.7680\nF1-Score: 0.8276\nAUC-ROC: 0.9747\n  \nRandom Forest (Tuned v1) Model Evaluation Metrics:\nAverage Accuracy: 0.9214\nPrecision: 0.9175\nRecall: 0.7230\nF1-Score: 0.8087\nAUC-ROC: 0.9683\n  \nRandom Forest (Tuned v2) Model Evaluation Metrics:\nAverage Accuracy: 0.9214\nPrecision: 0.9218\nRecall: 0.7190\nF1-Score: 0.8079\nAUC-ROC: 0.9684\n  \nXGBoost (Untuned) Model Evaluation Metrics:\nAverage Accuracy: 0.9375\nPrecision: 0.9098\nRecall: 0.7870\nF1-Score: 0.8440\nAUC-ROC: 0.9810\n  \nXGBoost (Tuned) Model Evaluation Metrics:\nAverage Accuracy: 0.9486\nPrecision: 0.9332\nRecall: 0.8240\nF1-Score: 0.8752\nAUC-ROC: 0.9868\n  \nKNN Model Evaluation Metrics:\nAverage Accuracy: 0.8678\nPrecision: 0.8947\nRecall: 0.4250\nF1-Score: 0.5763\nAUC-ROC: 0.8882"
  },
  {
    "objectID": "projects/data_project.html",
    "href": "projects/data_project.html",
    "title": "Coding and Visualisation",
    "section": "",
    "text": "The programming and visualisation module taught the basics of Python, SQL and Power BI. The module was split into two exams, one in Python and one in SQL, along with a group presentation for the Power BI portion. When it came to the SQL exam I scored in the high 90s and the high 70s for the Python exam. The Power BI group project was marked as an average of everyone, where my group was marked at a 70. The overall score I got for this module was 78, which was a very first class honours. The two pages I created on Power BI can be seen below:",
    "crumbs": [
      "Coding and Visualisation"
    ]
  },
  {
    "objectID": "projects/data_project.html#executive-summary",
    "href": "projects/data_project.html#executive-summary",
    "title": "Coding and Visualisation",
    "section": "Executive Summary",
    "text": "Executive Summary",
    "crumbs": [
      "Coding and Visualisation"
    ]
  },
  {
    "objectID": "projects/data_project.html#state-impact",
    "href": "projects/data_project.html#state-impact",
    "title": "Coding and Visualisation",
    "section": "State Impact",
    "text": "State Impact",
    "crumbs": [
      "Coding and Visualisation"
    ]
  },
  {
    "objectID": "projects/econometrics_model.html",
    "href": "projects/econometrics_model.html",
    "title": "Econometrics",
    "section": "",
    "text": "Throughout the Econometrics and Forecasting module, I have gained practical exposure to statistical analysis, regression modeling, and data-driven forecasting methods that are vital in modern finance. A core aspect of this coursework involves coding, particularly in Python, to clean and visualise datasets, build predictive models, and evaluate outcomes under various financial scenarios. These skills have proven invaluable in developing my individual project, ‚ÄúCUAN: Small Steps Today, Big Rewards Tomorrow,‚Äù where I applied these coding techniques to simulate pension contributions over a 45-year horizon.\nBy blending frameworks from behavioural economics, such as ‚ÄúSave More Tomorrow,‚Äù with the FIRE (Financial Independence, Retire Early) movement, the project demonstrates how data analytics and forecasting can illustrate the impact of systematic savings on retirement outcomes.\nBeyond theoretical exercises, the coding projects in this module mirror the demands of the finance industry: real-time data sourcing, complex model-building, and clear, evidence-based reporting. The Colab notebook I created validates CUAN‚Äôs higher returns compared to Ireland‚Äôs auto-enrolment system and showcases how effective coding practices, like iterative data cleaning and regression analysis, enhance both the precision and clarity of financial forecasts.\n\nOur Group Video\n\n\n\n\nMy Individual Assignment\n\n\n\n \n\nApp for Project",
    "crumbs": [
      "Econometrics"
    ]
  },
  {
    "objectID": "projects/pythonml.html",
    "href": "projects/pythonml.html",
    "title": "ML and Advanced Python",
    "section": "",
    "text": "During this module we learnt about the theory behind machine learning modules and why you would pick them in certain instances. This module was split into two midterm exams that were both worth 20% each and a report paper. Below you can see the report and the code for the project:\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Reflective Essay Code\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "ML and Advanced Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My ePortfolio üëã",
    "section": "",
    "text": "Here, you‚Äôll find a collection of my academic projects, achievements, and experiences from the past 4 years. This space showcases my work, interests, and insights, offering a glimpse into my journey and aspirations. Feel free to explore, connect with me on LinkedIn, or reach out if you have any questions, I‚Äôm always happy to chat!"
  },
  {
    "objectID": "about_me/dcu.html",
    "href": "about_me/dcu.html",
    "title": "Time at DCU üè´",
    "section": "",
    "text": "During my time at DCU, I have had the privilege of engaging in a diverse range of experiences that have shaped both my personal and professional growth. From competing on an international stage to taking on leadership roles within society committees, these opportunities have allowed me to develop valuable skills beyond the classroom.\nWhat made my time at DCU truly memorable was the combination of academic challenges and extracurricular involvement. Whether it was collaborating with peers on exciting projects, representing the university in competitions, or contributing to student societies, each experience has played a crucial role in broadening my perspective and enhancing my skill set.\nBelow, you will find a collection of the different opportunities I had the chance to be a part of. Feel free to explore and learn more about them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBIP Berlin, Germany üá©üá™\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaising and Giving Society üí∂\n\n\n\n\n\n\n\n\n\n\n\n\n\nSG-FECC Burlington, Vermont, USA üá∫üá∏\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching Tutorials üìö\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about_me/dcu_list/tutorials.html",
    "href": "about_me/dcu_list/tutorials.html",
    "title": "Teaching Tutorials üìö",
    "section": "",
    "text": "In the final year of my degree I had the opportunity to serve as a tutorial aid for both first‚Äë and second‚Äëyear business students, teaching them skills that I had learnt through my degree to help them with their assignments. I mainly focused on Excel and Power BI tutorials where I would take on some of the tutorials and help teach the students.\nWhen it came to Excel I taugh them how to run a Monte Carlo Simulation on a airlines flights over two years. During this I taught them how to run it with different probabilities for times, and different prices. These tutorials were taught mainly to Accounting and Finance second-years and Business Studies second-years.\nI also taught the first-years how to create a Power BI dashboard, where I showed them how to do it on an Irish Housing data set. Showing them how to remove outliers on Power BI without going into the original file and how to do change the different formats."
  },
  {
    "objectID": "Untitled-1.html",
    "href": "Untitled-1.html",
    "title": "Quarto Test",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n    subplot_kw = {'projection' : 'polar'}\n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Rory Mulhern",
    "section": "",
    "text": "I‚Äôm a final-year business student at Dublin City University, specialising in analytics. My hands-on exposure to credit analysis and investment reporting at KKR provided real-world insight into high-level finance and client interactions. Elected by peers as their representative for three consecutive years, I‚Äôve developed strong leadership, communication, and team-building skills.\nWith certifications in SQL, Python, and financial modeling, I blend analytical rigour with a people-focused approach. Upon graduation, I look forward to leveraging both my technical and interpersonal strengths in a dynamic environment within the finance industry.I‚Äôm a final-year business student at Dublin City University, specialising in analytics. My hands-on exposure to credit analysis and investment reporting at KKR provided real-world insight into high-level finance and client interactions. Elected by peers as their representative for three consecutive years, I‚Äôve developed strong leadership, communication, and team-building skills. With certifications in SQL, Python, and financial modeling, I blend analytical rigour with a people-focused approach. Upon graduation, I look forward to leveraging both my technical and interpersonal strengths in a dynamic environment within the finance industry.\n\n\n\n\n\n\n\n\n\n\n\n\nTime at DCU üè´\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime at KKR üíº\n\n\n\n\n\n\n\nNo matching items"
  }
]