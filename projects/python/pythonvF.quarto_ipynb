{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Machine Learning Reflective Essay\"\n",
        "theme:\n",
        "  light: lux\n",
        "sidebar: false\n",
        "jupyter: python3\n",
        "---"
      ],
      "id": "2173d871"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''\n",
        "ML Project for Module:\n",
        "BAA10127 - Data Analytics: Machine Learning & Advanced Python\n",
        "Student No. 21311696\n",
        "Student Name: Rory James Mulhern\n",
        "Course: BSI4\n",
        "Dataset: https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data\n",
        "'''"
      ],
      "id": "157a7ef1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "id": "aa212f52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Linking file to Code\n",
        "filepath = '/Users/mulhr/Desktop/ML Project/loan_data.csv'\n",
        "\n",
        "# Importing the dataset\n",
        "loans_df = pd.read_csv(filepath)\n",
        "\n",
        "loans_df"
      ],
      "id": "e50f7a28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Youssef Elbadry Accessed: 9th April 2025\n",
        "\n",
        "# Looking at info on the data\n",
        "loans_df.info()"
      ],
      "id": "0fc24903",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Changing the person_age column to an integer\n",
        "loans_df['person_age'] = loans_df['person_age'].astype(int)\n",
        "\n",
        "# Looking at info on the data\n",
        "loans_df.info()"
      ],
      "id": "12fc4daf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Removing duplicate rows\n",
        "loans_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check if there are any duplicates left\n",
        "duplicate_count = loans_df.duplicated().sum()\n",
        "\n",
        "# Display final check\n",
        "if duplicate_count == 0:\n",
        "    print(\"No duplicate values in the dataset.\")\n",
        "else:\n",
        "    print(f\"Total duplicate values remaining: {duplicate_count}\")"
      ],
      "id": "673d4e3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Looking at the data description see the statistics of numeric columns\n",
        "loans_df.describe().T"
      ],
      "id": "b77c43f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Youssef Elbadry Accessed: 9th April 2025\n",
        "\n",
        "# Seeing which columns are Categorical and Numerical\n",
        "cat_cols = [var for var in loans_df.columns if loans_df[var].dtypes == 'object']\n",
        "num_cols = [var for var in loans_df.columns if loans_df[var].dtypes != 'object']\n",
        "\n",
        "print(f'Categorical columns: {cat_cols}')\n",
        "print(f'Numerical columns: {num_cols}')"
      ],
      "id": "d12252e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cat_cols"
      ],
      "id": "9bea8423",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seeing the split in gender\n",
        "loans_df['person_gender'].value_counts()"
      ],
      "id": "5c15e642",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Youssef Elbadry Accessed: 9th April 2025\n",
        "def plot_categorical_column(dataframe, column):\n",
        "\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    ax = sns.countplot(x=dataframe[column])\n",
        "    total_count = len(dataframe[column])\n",
        "    threshold = 0.05 * total_count\n",
        "    category_counts = dataframe[column].value_counts(normalize=True) * 100\n",
        "    ax.axhline(threshold, color='red', linestyle='--', label=f'0.05% of total count ({threshold:.0f})')\n",
        "    \n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        percentage = (height / total_count) * 100\n",
        "        ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * total_count, f'{percentage:.2f}%', ha=\"center\")\n",
        "    \n",
        "    plt.title(f'Label Cardinality for \"{column}\" Column')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel(column)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for col in cat_cols:\n",
        "    plot_categorical_column(loans_df, col)"
      ],
      "id": "028fce7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df[num_cols].hist(bins=30, figsize=(12,10))\n",
        "plt.show()"
      ],
      "id": "6e29a382",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "label_prop = loans_df['loan_status'].value_counts()\n",
        "\n",
        "plt.pie(label_prop.values, labels=['Rejected (0)', 'Approved (1)'], autopct='%.2f')\n",
        "plt.title('Target label proportions')\n",
        "plt.show()"
      ],
      "id": "84d558d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''\n",
        "Article saying most lenders will not lend to anyone above 70\n",
        "https://www.moneysupermarket.com/loans/loans-for-pensioners/#:~:text=Most%20lenders%20have%20a%20maximum,beyond%20this%20age%20is%20rare.\n",
        "'''\n",
        "loans_df = loans_df[loans_df['person_age']<= 70]\n",
        "print('Ages above 70 removed!')"
      ],
      "id": "f031e96a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df[num_cols].hist(bins=30, figsize=(12,10))\n",
        "plt.show()"
      ],
      "id": "0c66fd5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sulani Ishara Accessed: 14th April 2025\n",
        "numerical_columns = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
        "fig.suptitle('Numerical Features vs Loan Status (Density Plots)', fontsize=16)\n",
        "\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    sns.kdeplot(data=loans_df, x=col, hue='loan_status', ax=axes[i//2, i%2], fill=True, common_norm=False, palette='muted')\n",
        "    axes[i//2, i%2].set_title(f'{col} vs Loan Status')\n",
        "    axes[i//2, i%2].set_xlabel(col)\n",
        "    axes[i//2, i%2].set_ylabel('Density')\n",
        "\n",
        "fig.delaxes(axes[3, 1])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "1130cd5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Box and Whisker plot to see what the outliers in the dataset look like\n",
        "# Sulani Ishara Accessed: 14th April 2025\n",
        "\n",
        "# Function to perform univariate analysis for numeric columns\n",
        "def univariate_analysis(data, column, title):\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    \n",
        "    sns.boxplot(x=data[column], color='sandybrown')\n",
        "    plt.title(f'{title} Boxplot')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'\\nSummary Statistics for {title}:\\n', data[column].describe())\n",
        "\n",
        "columns_to_analyse = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
        "\n",
        "for column in columns_to_analyse:\n",
        "    univariate_analysis(loans_df, column, column.replace('_', ' '))"
      ],
      "id": "8ef3656f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "for col in [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]:\n",
        "    loans_df[col] = winsorize(loans_df[col], limits=[0.025, 0.025])\n",
        "# Robust scaling\n",
        "scaler = RobustScaler()\n",
        "loans_df[[\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]] = scaler.fit_transform(loans_df[[\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]])\n",
        "\n",
        "# Box and Whisker plot to see what the outliers in the dataset look like\n",
        "# Function to perform univariate analysis for numeric columns\n",
        "\n",
        "for column in columns_to_analyse:\n",
        "    univariate_analysis(loans_df, column, column.replace('_', ' '))"
      ],
      "id": "f92c4ee6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columns_to_check = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\"]\n",
        "\n",
        "for col in columns_to_check:\n",
        "    skew_val = loans_df[col].skew()\n",
        "    print(f\"{col} skewness: {skew_val:.2f}\")"
      ],
      "id": "61b422f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply log1p directly — it's safe for 0s\n",
        "for col in columns_to_check:\n",
        "    loans_df[col] = np.log1p(loans_df[col])\n",
        "\n",
        "# Recheck skewness\n",
        "for col in columns_to_check:\n",
        "    skew_val = loans_df[col].skew()\n",
        "    print(f\"{col} skewness after log1p: {skew_val:.2f}\")\n",
        "\n",
        "for column in columns_to_analyse:\n",
        "    univariate_analysis(loans_df, column, column.replace('_', ' '))"
      ],
      "id": "6fd59e9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df\n",
        "loans_df.describe().T"
      ],
      "id": "cc049302",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sulani Ishara Accessed: 14th April 2025\n",
        "numerical_columns = ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
        "fig.suptitle('Numerical Features vs Loan Status (Density Plots)', fontsize=16)\n",
        "\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    sns.kdeplot(data=loans_df, x=col, hue='loan_status', ax=axes[i//2, i%2], fill=True, common_norm=False, palette='muted')\n",
        "    axes[i//2, i%2].set_title(f'{col} vs Loan Status')\n",
        "    axes[i//2, i%2].set_xlabel(col)\n",
        "    axes[i//2, i%2].set_ylabel('Density')\n",
        "\n",
        "fig.delaxes(axes[3, 1])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "f13267e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Making Education into a non-categorical columns\n",
        "loans_df['person_education'] = loans_df['person_education'].replace({\n",
        "    'High School': 0,\n",
        "    'Associate': 1,\n",
        "    'Bachelor': 2,\n",
        "    'Master': 3,\n",
        "    'Doctorate': 4\n",
        "})"
      ],
      "id": "4809f919",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans_df"
      ],
      "id": "7218c598",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-hot coding for dummy variables\n",
        "loans_df = pd.get_dummies(loans_df, columns = ['person_gender', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file'], drop_first = True)\n",
        "\n",
        "# Checking the data types\n",
        "loans_df.dtypes"
      ],
      "id": "b9e1ae6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define numerical columns with target\n",
        "numerical_columns_with_target = [\n",
        "    'person_age', \n",
        "    'person_income', \n",
        "    'person_emp_exp', \n",
        "    'loan_amnt', \n",
        "    'loan_int_rate', \n",
        "    'loan_percent_income', \n",
        "    'cb_person_cred_hist_length', \n",
        "    'credit_score'\n",
        "]\n",
        "\n",
        "# Create pairplot of numerical features with loan_status as hue\n",
        "sns.pairplot(loans_df[numerical_columns_with_target + ['loan_status']], \n",
        "             hue='loan_status', \n",
        "             palette='muted'\n",
        "            )\n",
        "plt.show()"
      ],
      "id": "bdc1c2ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting a correlation matrix\n",
        "num_loans_df = loans_df.select_dtypes(include=['number']) # Include only numerical data types\n",
        "\n",
        "# Correlation of that data\n",
        "corr_matrix = num_loans_df.corr()\n",
        "print(corr_matrix)"
      ],
      "id": "f694bca7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visual the Correlation Matrix\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Matrix of Variables')\n",
        "plt.show()"
      ],
      "id": "1de92e80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Drop Person Employment Experience and Age\n",
        "loans_df = loans_df.drop(columns=['person_emp_exp','person_age'])\n",
        "loans_df"
      ],
      "id": "6e5d15a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new column for custom labels\n",
        "loans_df['loan_status_label'] = loans_df['loan_status'].map({0: 'Denied (0)', 1: 'Approved (1)'})\n",
        "\n",
        "# Create a histogram plotting Approved and Denied loans\n",
        "sns.histplot(\n",
        "    data=loans_df,\n",
        "    x='loan_status_label',\n",
        "    hue='loan_status_label',\n",
        "    palette={\"Denied (0)\": \"red\", \"Approved (1)\": \"green\"}\n",
        ")\n",
        "plt.title(\"Amount of Denied and Approved Loans\")\n",
        "plt.xlabel(\"Loan Status\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show"
      ],
      "id": "9900043b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Splitting the Dataset into X and Y\n",
        "X = loans_df.drop(columns=['loan_status', 'loan_status_label'])  \n",
        "y = loans_df['loan_status'] "
      ],
      "id": "b5335b06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X"
      ],
      "id": "dfd2007b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y"
      ],
      "id": "fc659dbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Splitting the dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "id": "ccd313ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "id": "87af2059",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "lr_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    reg_model_lr = LogisticRegression(max_iter=200000, random_state=42)\n",
        "    reg_model_lr.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    lr_accuracy = reg_model_lr.score(X_test, y_test)\n",
        "    lr_accuracy_scores.append(lr_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {lr_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")"
      ],
      "id": "6ab998c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "lr2_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    reg_model_lr2 = LogisticRegression(max_iter=200000, random_state=42, penalty='l2')\n",
        "    reg_model_lr2.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    lr2_accuracy = reg_model_lr2.score(X_test, y_test)\n",
        "    lr2_accuracy_scores.append(lr2_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {lr2_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")"
      ],
      "id": "828e8400",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_lr = reg_model_lr.predict(X_test)"
      ],
      "id": "5c440996",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_lr2 = reg_model_lr2.predict(X_test)"
      ],
      "id": "37605a5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "lr_precision = precision_score(y_test, predictions_lr)\n",
        "lr_recall = recall_score (y_test, predictions_lr)\n",
        "lr_f1 = f1_score(y_test, predictions_lr)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {lr_precision:.4f}\")\n",
        "print(f\"Recall: {lr_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr_f1:.4f}\")"
      ],
      "id": "c1a66ed7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "lr2_precision = precision_score(y_test, predictions_lr2)\n",
        "lr2_recall = recall_score (y_test, predictions_lr2)\n",
        "lr2_f1 = f1_score(y_test, predictions_lr2)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {lr2_precision:.4f}\")\n",
        "print(f\"Recall: {lr2_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr2_f1:.4f}\")"
      ],
      "id": "765ed079",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lr_cm = confusion_matrix(y_test, predictions_lr )\n",
        "print(lr_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(lr_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (Logistic Regression)\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "63682019",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "lr_y_prob = reg_model_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "lr_auc_roc = roc_auc_score(y_test, lr_y_prob)\n",
        "print(f\"AUC-ROC: {lr_auc_roc:.4f}\")"
      ],
      "id": "900b637c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "lr2_y_prob = reg_model_lr2.predict_proba(X_test)[:, 1]\n",
        "\n",
        "lr2_auc_roc = roc_auc_score(y_test, lr2_y_prob)\n",
        "print(f\"AUC-ROC: {lr2_auc_roc:.4f}\")"
      ],
      "id": "ba77ff70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# From ChatGPT\n",
        "\n",
        "# Get false positive rate, true positive rate and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, lr_y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {lr_auc_roc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (Logistic Regression)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cd806463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "dt_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    dt_model = DecisionTreeClassifier(random_state=42)\n",
        "    dt_model.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    dt_accuracy = dt_model.score(X_test, y_test)\n",
        "    dt_accuracy_scores.append(dt_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {dt_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")"
      ],
      "id": "a45a2ce2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Decision Tree Model\n",
        "predictions_dt = dt_model.predict(X_test)"
      ],
      "id": "930ddedf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "dt_precision = precision_score(y_test, predictions_dt)\n",
        "dt_recall = recall_score (y_test, predictions_dt)\n",
        "dt_f1 = f1_score(y_test, predictions_dt)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {dt_precision:.4f}\")\n",
        "print(f\"Recall: {dt_recall:.4f}\")\n",
        "print(f\"F1-Score: {dt_f1:.4f}\")"
      ],
      "id": "607d7dd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dt_cm = confusion_matrix(y_test, predictions_dt )\n",
        "print(dt_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(dt_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (Decision Tree)\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "edd4048c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "dt_y_prob = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "dt_auc_roc = roc_auc_score(y_test, dt_y_prob)\n",
        "print(f\"AUC-ROC: {dt_auc_roc:.4f}\")"
      ],
      "id": "19466891",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# From ChatGPT\n",
        "\n",
        "# Get false positive rate, true positive rate and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, dt_y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {dt_auc_roc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (Decsision Tree)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e8b849c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "rf_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    rf_accuracy = rf_model.score(X_test, y_test)\n",
        "    rf_accuracy_scores.append(rf_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {rf_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")"
      ],
      "id": "9d26bedd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "rf2_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    rf2_model = RandomForestClassifier(n_estimators=200, \n",
        "                                       random_state=42, \n",
        "                                       max_depth=8,\n",
        "                                       min_samples_split=5,\n",
        "                                       min_samples_leaf=2,\n",
        "                                       max_features='sqrt',\n",
        "                                       bootstrap=True)\n",
        "    rf2_model.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    rf2_accuracy = rf2_model.score(X_test, y_test)\n",
        "    rf2_accuracy_scores.append(rf2_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {rf2_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")"
      ],
      "id": "5de0a8c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "rf3_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    rf3_model = RandomForestClassifier(n_estimators=200, \n",
        "                                       random_state=42, \n",
        "                                       max_depth=8,\n",
        "                                       min_samples_split=5,\n",
        "                                       min_samples_leaf=2,\n",
        "                                       max_features='sqrt',\n",
        "                                       bootstrap=False)\n",
        "    rf3_model.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    rf3_accuracy = rf3_model.score(X_test, y_test)\n",
        "    rf3_accuracy_scores.append(rf3_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {rf3_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")"
      ],
      "id": "ca8dbd2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_rf = rf_model.predict(X_test)"
      ],
      "id": "e8a92f04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_rf2 = rf2_model.predict(X_test)"
      ],
      "id": "8d1ab17f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_rf3 = rf3_model.predict(X_test)"
      ],
      "id": "6a789f36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "rf_precision = precision_score(y_test, predictions_rf)\n",
        "rf_recall = recall_score (y_test, predictions_rf)\n",
        "rf_f1 = f1_score(y_test, predictions_rf)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf_precision:.4f}\")\n",
        "print(f\"Recall: {rf_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf_f1:.4f}\")"
      ],
      "id": "3690c39d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "rf2_precision = precision_score(y_test, predictions_rf2)\n",
        "rf2_recall = recall_score (y_test, predictions_rf2)\n",
        "rf2_f1 = f1_score(y_test, predictions_rf2)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf2_precision:.4f}\")\n",
        "print(f\"Recall: {rf2_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf2_f1:.4f}\")"
      ],
      "id": "3415bdc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "rf3_precision = precision_score(y_test, predictions_rf3)\n",
        "rf3_recall = recall_score (y_test, predictions_rf3)\n",
        "rf3_f1 = f1_score(y_test, predictions_rf3)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf3_precision:.4f}\")\n",
        "print(f\"Recall: {rf3_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf3_f1:.4f}\")"
      ],
      "id": "a11e046f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_cm = confusion_matrix(y_test, predictions_rf)\n",
        "print(rf_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (Random Forest (Untuned))\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "dace621f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf2_cm = confusion_matrix(y_test, predictions_rf2)\n",
        "print(rf2_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(rf2_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (Random Forest (Tuned v1))\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "d0ef5d03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf3_cm = confusion_matrix(y_test, predictions_rf2)\n",
        "print(rf3_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(rf3_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (Random Forest (Tuned v2))\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "63b4a862",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "rf_y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "rf_auc_roc = roc_auc_score(y_test, rf_y_prob)\n",
        "print(f\"AUC-ROC: {rf_auc_roc:.4f}\")"
      ],
      "id": "04cbe9b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "rf2_y_prob = rf2_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "rf2_auc_roc = roc_auc_score(y_test, rf2_y_prob)\n",
        "print(f\"AUC-ROC: {rf2_auc_roc:.4f}\")"
      ],
      "id": "937fdc6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "rf3_y_prob = rf3_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "rf3_auc_roc = roc_auc_score(y_test, rf3_y_prob)\n",
        "print(f\"AUC-ROC: {rf3_auc_roc:.4f}\")"
      ],
      "id": "a707e984",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# From ChatGPT\n",
        "\n",
        "# Get false positive rate, true positive rate and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, rf_y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {rf_auc_roc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (Random Forest (Untuned))')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f218e26f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dictionary of model names and predicted probabilities\n",
        "models_probs = {\n",
        "    \"Random Forest(Untuned)\": rf_y_prob,\n",
        "    \"Random Forest(Tuned v1)\": rf2_y_prob,\n",
        "    \"Random Forest(Tuned v2)\": rf3_y_prob\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot each ROC curve\n",
        "for name, probs in models_probs.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.5f})')\n",
        "\n",
        "# Plot random guess line\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison of Models (Random Forest)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d639f581",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "xgb_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    xgb_accuracy = xgb_model.score(X_test, y_test)\n",
        "    xgb_accuracy_scores.append(xgb_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {xgb_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")"
      ],
      "id": "69f2216e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "xgb2_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    xgb2_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb2_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    xgb2_accuracy = xgb2_model.score(X_test, y_test)\n",
        "    xgb2_accuracy_scores.append(xgb2_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {xgb2_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")"
      ],
      "id": "bf2215a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_xgb = xgb_model.predict(X_test)"
      ],
      "id": "4e3e5685",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_xgb2 = xgb2_model.predict(X_test)"
      ],
      "id": "2e244a72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "xgb_precision = precision_score(y_test, predictions_xgb)\n",
        "xgb_recall = recall_score (y_test, predictions_xgb)\n",
        "xgb_f1 = f1_score(y_test, predictions_xgb)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {xgb_precision:.4f}\")\n",
        "print(f\"Recall: {xgb_recall:.4f}\")\n",
        "print(f\"F1-Score: {xgb_f1:.4f}\")"
      ],
      "id": "e28c29a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "xgb2_precision = precision_score(y_test, predictions_xgb2)\n",
        "xgb2_recall = recall_score (y_test, predictions_xgb2)\n",
        "xgb2_f1 = f1_score(y_test, predictions_xgb2)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {xgb2_precision:.4f}\")\n",
        "print(f\"Recall: {xgb2_recall:.4f}\")\n",
        "print(f\"F1-Score: {xgb2_f1:.4f}\")"
      ],
      "id": "c8082337",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xgb_cm = confusion_matrix(y_test, predictions_xgb)\n",
        "print(xgb_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(xgb_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (XGBoost (Untuned))\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "dc6435ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xgb2_cm = confusion_matrix(y_test, predictions_xgb2)\n",
        "print(xgb2_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(xgb2_cm, annot=True, fmt=\"d\", cmap=\"Blues\",  cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (XGBoost (Tuned))\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "63673930",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "xgb_y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "xgb_auc_roc = roc_auc_score(y_test, xgb_y_prob)\n",
        "print(f\"AUC-ROC: {xgb_auc_roc:.4f}\")"
      ],
      "id": "8902d45d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "xgb2_y_prob = xgb2_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "xgb2_auc_roc = roc_auc_score(y_test, xgb2_y_prob)\n",
        "print(f\"AUC-ROC: {xgb2_auc_roc:.4f}\")"
      ],
      "id": "50dc5f3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# From ChatGPT\n",
        "\n",
        "# Get false positive rate, true positive rate and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, xgb_y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {xgb_auc_roc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (XGBoost (Untuned))')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "a97e8f43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dictionary of model names and predicted probabilities\n",
        "models_probs = {\n",
        "    \"XGBoost (Tuned)\": xgb2_y_prob,\n",
        "    \"XGBoost (Untuned)\": xgb_y_prob,\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot each ROC curve\n",
        "for name, probs in models_probs.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.5f})')\n",
        "\n",
        "# Plot random guess line\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison of Models (XGBoost)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ae87c037",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting Up 10-Fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "knn_accuracy_scores = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    X_resampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_resampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # --- Model Training ---\n",
        "    knn_model = KNeighborsClassifier(\n",
        "        n_neighbors=2, \n",
        "        weights='uniform', \n",
        "        algorithm='auto', \n",
        "        leaf_size=30, \n",
        "        metric='minkowski'\n",
        "    )\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    knn_accuracy = knn_model.score(X_test, y_test)\n",
        "    knn_accuracy_scores.append(knn_accuracy)\n",
        "    print(f\"Fold {fold} Accuracy: {knn_accuracy:.4f}\")\n",
        "    \n",
        "print(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(knn_accuracy_scores):.4f}\")"
      ],
      "id": "8591b1a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Getting the predictions for the Logistic Regression Model\n",
        "predictions_knn = knn_model.predict(X_test)"
      ],
      "id": "c03c4859",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the evaluation metrics\n",
        "knn_precision = precision_score(y_test, predictions_knn)\n",
        "knn_recall = recall_score (y_test, predictions_knn)\n",
        "knn_f1 = f1_score(y_test, predictions_knn)\n",
        "\n",
        "# Print out evaluation metrics\n",
        "print(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {knn_precision:.4f}\")\n",
        "print(f\"Recall: {knn_recall:.4f}\")\n",
        "print(f\"F1-Score: {knn_f1:.4f}\")"
      ],
      "id": "b698eee7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_cm = confusion_matrix(y_test, predictions_knn)\n",
        "print(knn_cm)\n",
        "\n",
        "# Define new labels: index 0 -> \"Denied\", index 1 -> \"Approved\"\n",
        "labels = ['Denied', 'Approved']\n",
        "\n",
        "# Plot the confusion matrix heatmap with the renamed labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(knn_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Predicted Denied\", \"Predicted Approved\"],\n",
        "            yticklabels=[\"Actual Denied\", \"Actual Approved\"])\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix (KNN)\", fontsize=14)\n",
        "plt.show()"
      ],
      "id": "d6571eba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating the AUC-ROC | from one of the tutorials\n",
        "knn_y_prob = knn_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "knn_auc_roc = roc_auc_score(y_test, knn_y_prob)\n",
        "print(f\"AUC-ROC: {knn_auc_roc:.4f}\")"
      ],
      "id": "a1ffe7d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get false positive rate, true positive rate and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, knn_y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {knn_auc_roc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (KNN)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c3abce14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dictionary of model names and predicted probabilities\n",
        "models_probs = {\n",
        "    \"Logistic Regression\": lr_y_prob,\n",
        "    \"Decision Tree\": dt_y_prob,\n",
        "    \"Random Forest\": rf_y_prob,\n",
        "    \"Random Forest(Tuned v1)\": rf2_y_prob,\n",
        "    \"Random Forest(Tuned v2)\": rf3_y_prob,\n",
        "    \"XGBoost (Tuned)\": xgb2_y_prob,\n",
        "    \"XGBoost (Untuned)\": xgb_y_prob,\n",
        "    \"KNN\": knn_y_prob\n",
        "\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot each ROC curve\n",
        "for name, probs in models_probs.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Plot random guess line\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison of Models (All Models)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "b0983f70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print out all evaluation metrics\n",
        "print(\"Logistic Regression (Untuned) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(lr_accuracy_scores)/len(lr_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {lr_precision:.4f}\")\n",
        "print(f\"Recall: {lr_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {lr_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"Logistic Regression (Tuned) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(lr2_accuracy_scores)/len(lr2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {lr2_precision:.4f}\")\n",
        "print(f\"Recall: {lr2_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr2_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {lr2_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"Decision Tree Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(dt_accuracy_scores)/len(dt_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {dt_precision:.4f}\")\n",
        "print(f\"Recall: {dt_recall:.4f}\")\n",
        "print(f\"F1-Score: {dt_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {dt_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"Random Forest (Untuned) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(rf_accuracy_scores)/len(rf_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf_precision:.4f}\")\n",
        "print(f\"Recall: {rf_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {rf_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"Random Forest (Tuned v1) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(rf2_accuracy_scores)/len(rf2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf2_precision:.4f}\")\n",
        "print(f\"Recall: {rf2_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf2_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {rf2_auc_roc:.4f}\")\n",
        "\n",
        "print( \"  \")\n",
        "print(\"Random Forest (Tuned v2) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(rf3_accuracy_scores)/len(rf3_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {rf3_precision:.4f}\")\n",
        "print(f\"Recall: {rf3_recall:.4f}\")\n",
        "print(f\"F1-Score: {rf3_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {rf3_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"XGBoost (Untuned) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(xgb_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {xgb_precision:.4f}\")\n",
        "print(f\"Recall: {xgb_recall:.4f}\")\n",
        "print(f\"F1-Score: {xgb_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {xgb_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"XGBoost (Tuned) Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(xgb2_accuracy_scores)/len(xgb2_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {xgb2_precision:.4f}\")\n",
        "print(f\"Recall: {xgb2_recall:.4f}\")\n",
        "print(f\"F1-Score: {xgb2_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {xgb2_auc_roc:.4f}\")\n",
        "\n",
        "print(\"  \")\n",
        "print(\"KNN Model Evaluation Metrics:\")\n",
        "print(f\"Average Accuracy: {sum(knn_accuracy_scores)/len(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Precision: {knn_precision:.4f}\")\n",
        "print(f\"Recall: {knn_recall:.4f}\")\n",
        "print(f\"F1-Score: {knn_f1:.4f}\")\n",
        "print(f\"AUC-ROC: {knn_auc_roc:.4f}\")"
      ],
      "id": "215152b5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}